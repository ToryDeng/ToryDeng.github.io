<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://torydeng.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://torydeng.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-20T08:36:43+00:00</updated><id>https://torydeng.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Perron-Frobenius Theorem</title><link href="https://torydeng.github.io/blog/2023/perron-frobenius/" rel="alternate" type="text/html" title="The Perron-Frobenius Theorem"/><published>2023-12-08T00:00:00+00:00</published><updated>2023-12-08T00:00:00+00:00</updated><id>https://torydeng.github.io/blog/2023/perron-frobenius</id><content type="html" xml:base="https://torydeng.github.io/blog/2023/perron-frobenius/"><![CDATA[<p>The <a href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem">Perron-Frobenius Theorem</a> establishes powerful assertions about the eigenvalues and eigenvectors of certain types of matrices that are non-negative, which are incredibly insightful when dealing with dynamical systems, economics, demography, and beyond. This post offers an accessible proof of the theorem, which is carefully curated from lecture contents of CIE6002 Matrix Analysis.</p> <h2 id="notations">Notations</h2> <ul> <li>\(\boldsymbol{A} \in \mathbb{R}^{m \times m}\): an \(m\) by \(m\) matrix.</li> <li>\(\Vert \cdot \Vert\): in most cases it refers to matrix norm.</li> <li>\(\displaystyle \Vert \boldsymbol{A} \Vert_1 = \max_{j=1,2,\ldots,m} \sum_{i=1}^{m} \boldsymbol{A}_{ij}\): the maximum absolute column sum of the matrix.</li> <li>\(\displaystyle \Vert \boldsymbol{A} \Vert_\infty = \max_{i=1,2,\ldots,m} \sum_{j=1}^{m} \boldsymbol{A}_{ij}\): the maximum absolute row sum of the matrix.</li> </ul> <h2 id="the-theorem">The theorem</h2> <p>Let \(\boldsymbol{A} \in \mathbb{R}^{n \times n}\) be positive. That is, \(\boldsymbol{A}_{ij} &gt; 0, \forall 1 \le i,j \le m\). The <em>spectral radius</em> is defined as</p> \[\rho(\boldsymbol{A}) = \max_{i} |\lambda_i|\] <p>where \(\lambda_i\) is the \(i\)-th eigenvalue of \(\boldsymbol{A}\). Then</p> <ol> <li>\(\rho(\boldsymbol{A}) &gt; 0\), and \(\rho(\boldsymbol{A})\) is an eigenvalue of \(\boldsymbol{A}\);</li> <li>The corresponding eigenvector of \(\rho(\boldsymbol{A})\) is positive (or negative);</li> <li>\(\lvert \lambda \rvert &lt; \rho(\boldsymbol{A})\) for any \(\boldsymbol{A}\)’s eigenvalue \(\lambda \ne \rho(\boldsymbol{A})\);</li> <li>\(\operatorname{dim}(\operatorname{Null}(\boldsymbol{A} - \rho(\boldsymbol{A})\boldsymbol{I})) = 1\).</li> </ol> <h2 id="some-lemmas">Some lemmas</h2> <p><strong>Lemma 1:</strong> \(\rho(\boldsymbol{A}) \le \Vert \boldsymbol{A} \Vert\) for any matrix norm \(\Vert\cdot\Vert.\)</p> <p><strong>Proof:</strong> Let \(\boldsymbol{Av} = \lambda \boldsymbol{v}\) with \(\lvert\lambda\rvert = \rho(\boldsymbol{A})\). Let \(\boldsymbol{V} = \boldsymbol{v} \boldsymbol{1}^\top \in \mathbb{R}^{m \times m}\). Then</p> \[\begin{align*} &amp;\boldsymbol{AV} = \lambda \boldsymbol{V} \\ \Rightarrow &amp;\Vert\boldsymbol{AV}\Vert = \lvert \lambda \rvert \cdot \Vert\boldsymbol{V}\Vert \le \Vert\boldsymbol{A}\Vert \cdot \Vert\boldsymbol{V}\Vert \\ \Rightarrow &amp; \lvert \lambda \rvert = \rho(\boldsymbol{A}) \le \Vert\boldsymbol{A}\Vert. \end{align*}\] <hr/> <p><strong>Lemma 2:</strong> Given \(\varepsilon &gt; 0\). There exists a matrix norm \(\Vert\cdot\Vert\) s.t. \(\rho(\boldsymbol{A}) \le \Vert\boldsymbol{A}\Vert \le \rho(\boldsymbol{A}) + \varepsilon \Rightarrow \rho(\boldsymbol{A}) = \inf_{\Vert\cdot\Vert} \Vert\boldsymbol{A}\Vert\).</p> <p><strong>Proof:</strong> The Schur triangularization of \(\boldsymbol{A}\) is \(\boldsymbol{A} = \boldsymbol{UTU}^\top\), where \(\boldsymbol{U}\) is unitary and diagonals \(\lambda_1, \ldots, \lambda_m\) of \(\boldsymbol{T}\) are eigenvalues of \(\boldsymbol{A}\). Define</p> \[\begin{align*} \Vert\boldsymbol{A}\Vert &amp;\triangleq \Vert(\boldsymbol{UD}_t^{-1})^{-1}\boldsymbol{A}(\boldsymbol{UD}_t^{-1})\Vert_1 \\ &amp;= \Vert\boldsymbol{D}_t \boldsymbol{U}^\top \boldsymbol{AUD}_t^{-1}\Vert_1 \\ &amp;= \Vert\boldsymbol{D}_t \boldsymbol{T} \boldsymbol{D}_t^{-1}\Vert_1 \\ &amp;= \Vert\begin{bmatrix} \lambda_1 &amp; t^{-1}T_{12} &amp; t^{-2}T_{13} &amp; \cdots &amp; t^{-m+1}T_{1m}\\ &amp; \lambda_2 &amp; t^{-1}T_{13} &amp; \cdots &amp; t^{-m+2}T_{2m}\\ &amp; &amp; \lambda_3 &amp; \cdots &amp; t^{-m+3}T_{3m} \\ &amp; &amp; &amp; \ddots &amp; \vdots \\ &amp; &amp; &amp; &amp; \lambda_m \end{bmatrix}\Vert_1 \\ &amp;\le \rho(\boldsymbol{A}) + \varepsilon \quad \text{for large}\ t \end{align*}\] <p>where \(\boldsymbol{D}_t = \begin{bmatrix} t^1 &amp; &amp; &amp; \\ &amp; t^2 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; t^m \end{bmatrix}.\)</p> <hr/> <p><strong>Lemma 3:</strong> \(\lim_{k \to \infty} \boldsymbol{A}^k = \boldsymbol{0} \iff \rho(\boldsymbol{A}) &lt; 1.\)</p> <p><strong>Proof:</strong> The “\(\Rightarrow\)” part. Let \(\boldsymbol{Av} = \lambda \boldsymbol{v}\) for any eigenvalue \(\lambda\) of \(\boldsymbol{A}\). Then</p> \[\boldsymbol{A}^k\boldsymbol{v} = \lambda^k \boldsymbol{v} \Rightarrow \lambda^k \to 0 \Rightarrow |\lambda| &lt; 1 \Rightarrow \rho(\boldsymbol{A}) &lt; 1.\] <p>The “\(\Leftarrow\)” part. By Lemma 2, \(\exists\ \Vert\cdot\Vert\) s.t. \(\Vert\boldsymbol{A}\Vert &lt; 1.\) Then</p> \[0 \le \lim_{k \to \infty} \Vert\boldsymbol{A}^k\Vert \le \lim_{k \to \infty} \Vert\boldsymbol{A}\Vert^k = 0 \Rightarrow \lim_{k \to \infty} \boldsymbol{A}^k = \boldsymbol{0}.\] <hr/> <p><strong>Lemma 4:</strong> Let \(\boldsymbol{A}, \boldsymbol{B} \in \mathbb{R}^{m \times m}\) and \(\vert \boldsymbol{A} \vert \le \boldsymbol{B}\) element-wise. Then</p> \[\rho(\boldsymbol{A}) \le \rho(\boldsymbol{\vert A \vert}) \le \rho(\boldsymbol{B}).\] <p><strong>Proof:</strong></p> \[\begin{align*} &amp;\boldsymbol{A} \le \boldsymbol{\vert A \vert} \le \boldsymbol{B} \\ \Rightarrow&amp; \boldsymbol{A}^k \le \boldsymbol{\vert A \vert}^k \le \boldsymbol{B}^k \\ \Rightarrow&amp; \Vert \boldsymbol{A}^k \Vert_F \le \Vert \boldsymbol{\vert A \vert}^k \Vert_F \le \Vert \boldsymbol{B}^k \Vert_F \\ \Rightarrow&amp; \Vert \boldsymbol{A}^k \Vert_F^{1/k} \le \Vert \boldsymbol{\vert A \vert}^k \Vert_F^{1/k} \le \Vert \boldsymbol{B}^k \Vert_F^{1/k} \\ \Rightarrow&amp; \rho(\boldsymbol{A}) \le \rho(\boldsymbol{\vert A \vert}) \le \rho(\boldsymbol{B}). \end{align*}\] <p>The last step is given by Theorem 1.</p> <p><strong>Corollary 4.1:</strong> Let \(\boldsymbol{A} \ge \boldsymbol{0}\) element-wise. Then for any principal submatrix of \(\boldsymbol{A}\), denoted as \(\tilde{\boldsymbol{A}}\), we have \(\rho(\tilde{\boldsymbol{A}}) \le \rho(\boldsymbol{A}) \Rightarrow \max_{i} \boldsymbol{A}_{ii} \le \rho(\boldsymbol{A})\).</p> <hr/> <p><strong>Lemma 5:</strong> Let \(\boldsymbol{A} \ge \boldsymbol{0}\) element-wise. If row (column) sums of \(\boldsymbol{A}\) are constant, then \(\rho(\boldsymbol{A}) = \Vert \boldsymbol{A} \Vert_\infty\) ($\rho(\boldsymbol{A}) = \Vert \boldsymbol{A} \Vert_1$).</p> <p><strong>Proof:</strong> Suppose \(\boldsymbol{A1} = \alpha \boldsymbol{1}\) where \(\alpha \ge 0\) is the row sum, and thus is an eigenvalue of \(\boldsymbol{A}\). So \(\alpha \le \rho(\boldsymbol{A})\). But \(\alpha = \Vert \boldsymbol{A} \Vert_\infty \ge \rho(\boldsymbol{A})\), so \(\rho(\boldsymbol{A}) = \Vert \boldsymbol{A} \Vert_\infty.\) The column sum case is similar.</p> <hr/> <p><strong>Lemma 6:</strong> Let \(\boldsymbol{A} \ge \boldsymbol{0}\) element-wise. Then</p> \[\min_{i = 1, \ldots, m} \sum_{j = 1}^m \boldsymbol{A}_{ij} \le \rho(\boldsymbol{A}) \le \Vert \boldsymbol{A} \Vert_\infty,\ \min_{j = 1, \ldots, m} \sum_{i = 1}^m \boldsymbol{A}_{ij} \le \rho(\boldsymbol{A}) \le \Vert \boldsymbol{A} \Vert_1\] <p><strong>Proof:</strong> Let</p> \[\alpha = \min_{i = 1, \ldots, m} \sum_{j = 1}^m \boldsymbol{A}_{ij} \ne 0.\] <p>\(\alpha = 0\) is a trivial case, so assume \(\alpha \ne 0\). Construct a new matrix \(\boldsymbol{B}\) by multiplying \(\alpha / \sum_{j=1}^m \boldsymbol{A}_{ij}\) to each \(i\)-th row of \(\boldsymbol{A}\).</p> <p>So \(\boldsymbol{0} \le \boldsymbol{B} \le \boldsymbol{A}\) element-wise, and \(\boldsymbol{B}\) has a constant row sum equal to \(\alpha \Rightarrow \alpha = \rho(\boldsymbol{B}) \le \rho(\boldsymbol{A})\) by Lemma 4 and 5. \(\rho(\boldsymbol{A}) \le \Vert \boldsymbol{A} \Vert_\infty\) by Lemma 1. The column sum case is similar.</p> <hr/> <p><strong>Lemma 7:</strong> Let \(\boldsymbol{A} &gt; \boldsymbol{0}\) element-wise. Suppose \(\boldsymbol{Ax} = \lambda \boldsymbol{x}\), and \(\vert \lambda \vert = \rho(\boldsymbol{A})\). Then \(\exists\ \theta \in \mathbb{R}\) s.t. \(e^{-j\theta} \boldsymbol{x} = \vert \boldsymbol{x} \vert &gt; \boldsymbol{0}\) element-wise. Here \(j = \sqrt{-1}\).</p> <p><strong>Proof:</strong> \(\vert \boldsymbol{Ax} \vert = \vert \lambda \vert \vert \boldsymbol{x} \vert = \rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert\). By Theorem 3, \(\boldsymbol{A} \vert \boldsymbol{x} \vert = \rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert\). So \(\vert \boldsymbol{Ax} \vert = \boldsymbol{A} \vert \boldsymbol{x} \vert\) and \(\vert \boldsymbol{x} \vert &gt; \boldsymbol{0}\). For any \(1 \le i \le m\),</p> \[[\vert \boldsymbol{Ax} \vert]_i = \left\vert \sum_{j=1}^m \boldsymbol{A}_{ik} \boldsymbol{x}_k \right\vert = \sum_{k=1}^m \boldsymbol{A}_{ik} \vert \boldsymbol{x}_k \vert\] <p>For any \(\boldsymbol{x}_k \in \mathbb{C}, \boldsymbol{x}_k = \vert \boldsymbol{x}_k \vert e^{j\theta_k} = \vert \boldsymbol{x}_k \vert \cos(\theta_k) + \vert \boldsymbol{x}_k \vert \sin(\theta_k) \cdot j\). So</p> \[\begin{align*} &amp;\left\vert \sum_{j=1}^m \boldsymbol{A}_{ik} \boldsymbol{x}_k \right\vert = \sum_{k=1}^m \boldsymbol{A}_{ik} \vert \boldsymbol{x}_k \vert \\ \Rightarrow&amp; \left(\sum_{j=1}^m \boldsymbol{A}_{ik} \boldsymbol{x}_k\right) e^{-j\theta} = \sum_{k=1}^m \boldsymbol{A}_{ik} \boldsymbol{x}_k e^{-j\theta_k} \\ \Rightarrow&amp; \sum_{j=1}^m \boldsymbol{A}_{ik} \boldsymbol{x}_k e^{-j\theta} = \sum_{k=1}^m \boldsymbol{A}_{ik} \boldsymbol{x}_k e^{-j\theta_k} \\ \Rightarrow&amp; \theta_k = \theta, k=1, \ldots, m. \end{align*}\] <h2 id="some-theorems">Some theorems</h2> <p><strong>Theorem 1:</strong> \(\rho(\boldsymbol{A}) = \lim_{k \to \infty} \Vert \boldsymbol{A}^k \Vert^{1/k}\) for any matrix norm.</p> <p><strong>Proof:</strong> The aim is to show \(0 \le \Vert \boldsymbol{A}^k \Vert^{1/k} - \rho(\boldsymbol{A}) \le \varepsilon\) for large \(k\).</p> <p>To see this, \(\rho^k(\boldsymbol{A}) = \rho(\boldsymbol{A^k}) \le \Vert \boldsymbol{A}^k \Vert\) by Lemma 2. Thus \(\rho(\boldsymbol{A}) \le \Vert \boldsymbol{A}^k \Vert^{1/k} \Rightarrow 0 \le \Vert \boldsymbol{A}^k \Vert^{1/k} - \rho(\boldsymbol{A})\).</p> <p>Let \(\tilde{\boldsymbol{A}} = \frac{1}{\varepsilon + \rho(\boldsymbol{A})} \boldsymbol{A}\). It’s easy to see \(\rho(\tilde{\boldsymbol{A}}) &lt; 1\). Then for \(k\) large enough \(\Vert \tilde{\boldsymbol{A}}^k \Vert \le 1 \Leftrightarrow \frac{1}{(\varepsilon + \rho(\boldsymbol{A}))^k} \Vert \boldsymbol{A}^k \Vert \le 1 \Leftrightarrow \Vert \boldsymbol{A}^k \Vert^{1/k} \le \varepsilon + \rho(\boldsymbol{A}).\)</p> <hr/> <p><strong>Theorem 2:</strong> For any \(\boldsymbol{A} \ge \boldsymbol{0}\) element-wise and \(\boldsymbol{x} &gt; \boldsymbol{0}\) element-wise,</p> \[\min_{i = 1, \ldots, m} \frac{1}{\boldsymbol{x}_i}\sum_{j=1}^m \boldsymbol{A}_{ij} \boldsymbol{x}_j \le \rho(\boldsymbol{A}) \le \max_{i = 1, \ldots, m} \frac{1}{\boldsymbol{x}_i}\sum_{j=1}^m \boldsymbol{A}_{ij} \boldsymbol{x}_j\] <p><strong>Proof:</strong> Define \(\bar{\boldsymbol{A}} \triangleq \boldsymbol{S}^{-1}\boldsymbol{A}\boldsymbol{S}\), where</p> \[\boldsymbol{S} = \begin{bmatrix} \boldsymbol{x}_1 &amp; &amp; &amp; \\ &amp; \boldsymbol{x}_2 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; \boldsymbol{x}_m \end{bmatrix},\ \boldsymbol{S}^{-1} = \begin{bmatrix} \frac{1}{\boldsymbol{x}_1} &amp; &amp; &amp; \\ &amp; \frac{1}{\boldsymbol{x}_2} &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; \frac{1}{\boldsymbol{x}_m} \end{bmatrix}.\] <p>\(\rho(\bar{\boldsymbol{A}}) = \rho(\boldsymbol{A})\) as \(\bar{\boldsymbol{A}}\) and \(\boldsymbol{A}\) have same eigenvalues. Apply Lemma 6 to \(\bar{\boldsymbol{A}}\).</p> <p><strong>Corollary 2.1:</strong> For any \(\boldsymbol{A} \ge \boldsymbol{0}\) element-wise and \(\boldsymbol{x} &gt; \boldsymbol{0}\) element-wise, if</p> \[$\alpha \boldsymbol{x} \le \boldsymbol{Ax} \le \beta \boldsymbol{x}\] <p>where \(\alpha, \beta \ge 0\). Then \(\alpha \le \rho(\boldsymbol{A}) \le \beta\). If the inequality is strict, \(\alpha &lt; \rho(\boldsymbol{A}) &lt; \beta\).</p> <p><strong>Proof:</strong></p> \[\begin{align*} \alpha \boldsymbol{x}_i \le \sum_{j=1}^m \boldsymbol{A}_{ij}\boldsymbol{x}_j \Rightarrow \alpha \le \frac{1}{\boldsymbol{x}_i} \sum_{j=1}^m \boldsymbol{A}_{ij}\boldsymbol{x}_j \Rightarrow \alpha \le \min_{i = 1, \ldots, m} \frac{1}{\boldsymbol{x}_i}\sum_{j=1}^m \boldsymbol{A}_{ij} \boldsymbol{x}_j \le \rho(\boldsymbol{A}). \end{align*}\] <p>\(\rho(\boldsymbol{A}) \le \beta\) is similar.</p> <p><strong>Corollary 2.2:</strong> If \(\boldsymbol{A} \ge \boldsymbol{0}\) element-wise and has a eigenvector \(\boldsymbol{x} &gt; \boldsymbol{0}\) element-wise. Then the associated eigenvalue must be \(\rho(\boldsymbol{A})\).</p> <p><strong>Proof:</strong> \(\lambda \boldsymbol{x} \le \boldsymbol{Ax} \le \lambda \boldsymbol{x} \Rightarrow \lambda \le \rho(\boldsymbol{A}) \le \lambda \Rightarrow \rho(\boldsymbol{A}) = \lambda.\)</p> <hr/> <p><strong>Theorem 3:</strong> Let \(\boldsymbol{A} &gt; \boldsymbol{0}\) element-wise. Suppose \(\boldsymbol{Ax} = \lambda \boldsymbol{x}\) for some \(\lambda \in \mathbb{R}, \boldsymbol{x} \in \mathbb{R}^{m}\), and \(\vert \lambda \vert = \rho(\boldsymbol{A})\). Then \(\boldsymbol{A} \vert \boldsymbol{x} \vert = \rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert\) and \(\vert \boldsymbol{x} \vert &gt; \boldsymbol{0}\).</p> <p><strong>Proof:</strong> By Corollary 4.1, \(0 &lt; \max_i \boldsymbol{A}_{ii} \le \rho(\boldsymbol{A})\). \(\boldsymbol{Ax} = \lambda\boldsymbol{x} \Rightarrow \vert \boldsymbol{Ax} \vert = \vert \lambda \vert \vert \boldsymbol{x} \vert = \rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert \le \boldsymbol{A} \vert \boldsymbol{x} \vert.\) Define \(\boldsymbol{y} \triangleq \boldsymbol{A} \vert \boldsymbol{x} \vert - \rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert \ge \boldsymbol{0}\) element-wise.</p> <ul> <li>If \(\boldsymbol{y} \equiv \boldsymbol{0}\), i.e., \(\rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert = \boldsymbol{A} \vert \boldsymbol{x} \vert\) element-wise: since \(\boldsymbol{A} \vert \boldsymbol{x} \vert &gt; \boldsymbol{0}\) element-wise and \(0 &lt; \rho(\boldsymbol{A})\), \(\vert \boldsymbol{x} \vert &gt; \boldsymbol{0}\) element-wise.</li> <li>If \(\boldsymbol{y}_i &gt; 0\) for some \(i\), let \(\boldsymbol{z} \triangleq \boldsymbol{A} \vert \boldsymbol{x} \vert &gt; \boldsymbol{0}\) element-wise. Then</li> </ul> \[\begin{align*} &amp;\boldsymbol{0} &lt; \boldsymbol{Ay} = \boldsymbol{A}(\boldsymbol{A} \vert \boldsymbol{x} \vert - \rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert) = \boldsymbol{Az} - \rho(\boldsymbol{A}) \boldsymbol{z} \\ \Rightarrow&amp; \rho(\boldsymbol{A}) \boldsymbol{z} &lt; \boldsymbol{Az} \\ \Rightarrow&amp; \rho(\boldsymbol{A}) &lt; \rho(\boldsymbol{A})\qquad \text{by Corollary 2.1} \end{align*}\] <p>which is a contradiction. So \(\boldsymbol{y} \equiv \boldsymbol{0}\).</p> <hr/> <p><strong>Theorem 4:</strong> Let \(\boldsymbol{A} &gt; \boldsymbol{0}\) element-wise. Then \(\forall\ \lambda \ne \rho(\boldsymbol{A}), \vert \lambda \vert &lt; \rho(\boldsymbol{A})\).</p> <p><strong>Proof:</strong> Suppose \(\exists\ \lambda \ne \rho(\boldsymbol{A})\) but \(\vert \lambda \vert = \rho(\boldsymbol{A})\) and \(\boldsymbol{Ax} = \lambda \boldsymbol{x}\). By Lemma 7, \(\exists\ \theta \in \mathbb{R}\) s.t. \(\vert \boldsymbol{x} \vert = e^{-j\theta} \boldsymbol{x} &gt; \boldsymbol{0}\). \(\boldsymbol{Ax} = \lambda \boldsymbol{x} \Rightarrow \boldsymbol{A} \vert \boldsymbol{x} \vert = \lambda \vert \boldsymbol{x} \vert\). By Corollary 2.2 \(\lambda = \rho(\boldsymbol{A})\), which is a contradiction.</p> <hr/> <p><strong>Theorem 5:</strong> Let \(\boldsymbol{A} &gt; \boldsymbol{0}\) element-wise. Suppose for two vectors \(\boldsymbol{w}, \boldsymbol{z}\) s.t. \(\boldsymbol{Aw} = \rho(\boldsymbol{A})\boldsymbol{w}, \boldsymbol{Az} = \rho(\boldsymbol{A})\boldsymbol{z}\). Then \(\exists\ \alpha \in \mathbb{C}\) s.t. \(\boldsymbol{w} = \alpha \boldsymbol{z}\), i.e., \(\operatorname{dim}(\operatorname{Null}(\boldsymbol{A} - \rho(\boldsymbol{A})\boldsymbol{I})) = 1\).</p> <p><strong>Proof:</strong> By Lemma 7, \(\exists\ \theta_1, \theta_2\) s.t. \(\boldsymbol{q} = \vert \boldsymbol{w} \vert = e^{-j\theta_1} \boldsymbol{w} &gt; \boldsymbol{0}, \boldsymbol{p} = \vert \boldsymbol{z} \vert = e^{-j\theta_2} \boldsymbol{z} &gt; \boldsymbol{0}\). By Theorem 3 \(\boldsymbol{Aq} = \rho(\boldsymbol{A})\boldsymbol{q}, \boldsymbol{Ap} = \rho(\boldsymbol{A})\boldsymbol{p}\). Let \(\beta = \min_{i=1,\ldots, m} \frac{\boldsymbol{q}_i}{\boldsymbol{p}_i}\) and \(\boldsymbol{r} = \boldsymbol{q} - \beta \boldsymbol{p} \ge \boldsymbol{0}\) with \(\boldsymbol{r}_j = 0\) for some \(j\). Then</p> \[\begin{align*} \boldsymbol{Ar} &amp;= \boldsymbol{Aq} - \beta \boldsymbol{Ap}\\ &amp;= \rho(\boldsymbol{A})\boldsymbol{q} - \beta \rho(\boldsymbol{A})\boldsymbol{p}\\ &amp;= \rho(\boldsymbol{A})\boldsymbol{r} \end{align*}\] <ul> <li>If \(\boldsymbol{r} \equiv \boldsymbol{0}\), then \(\boldsymbol{q} = \beta \boldsymbol{p}\).</li> <li>If \(\boldsymbol{r}_k &gt; 0\) for some \(k\), then \(\boldsymbol{Ar} = \rho(\boldsymbol{A})\boldsymbol{r}&gt;\boldsymbol{0} \Rightarrow \boldsymbol{r} &gt; \boldsymbol{0}\) which is a contradiction.</li> </ul> <p>So \(\boldsymbol{r} = \boldsymbol{0} \Rightarrow \boldsymbol{q} = \beta \boldsymbol{p} \Rightarrow \boldsymbol{w} = \alpha \boldsymbol{z}\).</p> <h2 id="an-application">An application</h2> <p><strong>Irreducible matrix:</strong> Let \(\boldsymbol{A} \in \mathbb{R}^{m \times m}, \boldsymbol{A} \ge \boldsymbol{0}\) element-wise. \(\boldsymbol{A}\) is <em>irreducible</em> if for each index \((i, j), \exists\ k \in \mathbb{N}^+\) s.t. \([\boldsymbol{A}^k]_{ij} &gt; 0\).</p> <p><strong>Subinvariance Theorem:</strong> Let \(\boldsymbol{A} \ge \boldsymbol{0}\) be irreducible. Suppose for some \(\boldsymbol{y} \ge \boldsymbol{0}, \boldsymbol{y} \ne \boldsymbol{0}\) and \(s &gt; 0\), we have \(\boldsymbol{Ay} \le s\boldsymbol{y}\). Then</p> <ol> <li>\(\boldsymbol{y} &gt; \boldsymbol{0}\);</li> <li>\(\rho(\boldsymbol{A}) \le s\);</li> <li>\(\rho(\boldsymbol{A}) = s \iff \boldsymbol{Ay} = s\boldsymbol{y}\).</li> </ol> <p><strong>Proof:</strong></p> <ol> <li>Suppose \(\boldsymbol{y}_i = 0\) for some \(i\), then \(0 \le [\boldsymbol{Ay}]_i \le s \cdot 0 = 0 \Rightarrow [\boldsymbol{Ay}]_i = 0\). Let \(\boldsymbol{z} \triangleq \boldsymbol{Ay}\), then \(\boldsymbol{Az} \le s \boldsymbol{Ay} = s \boldsymbol{z}\). As \(\boldsymbol{z}_i = [\boldsymbol{Ay}]_i = 0, [\boldsymbol{Az}]_i = 0\). Repeat this and we get \([\boldsymbol{A}^k\boldsymbol{y}]_i = 0\) for any \(k\). But for \(k\) large enough \(\boldsymbol{A}^k &gt; \boldsymbol{0}\) which is a contradiction.</li> <li>Corollary 2.1.</li> <li>The “$\Leftarrow$” part is from Corollary 2.1 so we only need to prove the “$\Rightarrow$” part. Suppose \([\boldsymbol{Ay}]_i &lt; s \boldsymbol{y}_i\) for some \(i\). Define \(\boldsymbol{z} = s \boldsymbol{y} - \boldsymbol{Ay} \ge \boldsymbol{0}\) and \(\boldsymbol{z} \ne \boldsymbol{0}\). For \(k\) large enough, \(\boldsymbol{A}^k \boldsymbol{z} = s \boldsymbol{A}^k\boldsymbol{y} - \boldsymbol{A}^k\boldsymbol{Ay} = s \boldsymbol{x} - \boldsymbol{Ax} &gt; \boldsymbol{0}\) where \(\boldsymbol{x} = \boldsymbol{A}^k \boldsymbol{y} &gt; \boldsymbol{0}\). So \(\boldsymbol{Ax} &lt; s \boldsymbol{x} \Rightarrow \rho(\boldsymbol{A}) &lt; s = \rho(\boldsymbol{A})\) by Corollary 2.1, which is a contradiction.</li> </ol> <p><strong>Power control in wireless network:</strong> \(\boldsymbol{A} \ge \boldsymbol{0}\) and is irreducible. \(\boldsymbol{p}, \boldsymbol{b} \in \mathbb{R}^m\) and \(\boldsymbol{b} \ge \boldsymbol{0}\). Suppose \(\boldsymbol{A}, \boldsymbol{b}\) are known, then</p> \[\begin{cases} \boldsymbol{Ap} + \boldsymbol{b} \le \boldsymbol{p} \\ \boldsymbol{p} \ge \boldsymbol{0} \end{cases}\] <p>is feasible w.r.t \(\boldsymbol{p} \iff \rho(\boldsymbol{A}) &lt; 1\).</p> <p><strong>Proof:</strong> The “\(\Leftarrow\)” part. We show \((\boldsymbol{I} - \boldsymbol{A})^{-1}\) exists and \((\boldsymbol{I} - \boldsymbol{A})^{-1} &gt; \boldsymbol{0}\) element-wise, so \(\boldsymbol{p} = (\boldsymbol{I} - \boldsymbol{A})^{-1}\boldsymbol{b}\) is a feasible point.</p> \[(\boldsymbol{I} - \boldsymbol{A}) \lim_{k \to \infty} \sum_{i=0}^k \boldsymbol{A}^k = \lim_{k \to \infty} (\boldsymbol{I} - \boldsymbol{A}^{k+1}) = \boldsymbol{I}.\] <p>\(\lim_{k \to \infty} \boldsymbol{A}^{k+1} = \boldsymbol{0}\) by Lemma 3. So \((\boldsymbol{I} - \boldsymbol{A})^{-1} = \lim_{k \to \infty} \sum_{i=0}^k \boldsymbol{A}^k &gt; \boldsymbol{0}\) since \(\boldsymbol{A}\) is irreducible.</p> <p>The “\(\Rightarrow\)” part. As \(\boldsymbol{b} \ge \boldsymbol{0}\), we have \(\boldsymbol{Ap} \le \boldsymbol{p}, \boldsymbol{p} \ge \boldsymbol{0}\). By Subinvariance Theorem, \(\boldsymbol{p} &gt; \boldsymbol{0}\) and \(\rho(\boldsymbol{A}) \le 1\). If \(\rho(\boldsymbol{A}) = 1\), then \(\boldsymbol{Ap} = \boldsymbol{p}\) which is a contradiction as \(\boldsymbol{b} \ne \boldsymbol{0}\). So \(\rho(\boldsymbol{A}) &lt; 1\).</p>]]></content><author><name></name></author><category term="mathematics"/><category term="matrix"/><category term="eigenvalue"/><category term="spectral-radius"/><summary type="html"><![CDATA[a detailed proof of the PF theorem and an application]]></summary></entry><entry><title type="html">Deploying a Server for Bioinformatics Research</title><link href="https://torydeng.github.io/blog/2023/deploying-server/" rel="alternate" type="text/html" title="Deploying a Server for Bioinformatics Research"/><published>2023-10-14T00:00:00+00:00</published><updated>2023-10-14T00:00:00+00:00</updated><id>https://torydeng.github.io/blog/2023/deploying-server</id><content type="html" xml:base="https://torydeng.github.io/blog/2023/deploying-server/"><![CDATA[<p>Recently, our lab acquired a server equipped with a standard Ubuntu operating system from <code class="language-plaintext highlighter-rouge">Inspur</code>, and I am tasked with configuring it to fulfill the specific requirements of our bioinformatics research. Given that my expertise in Linux is limited, I dedicated several days to this endeavor, and eventually completed the deployment process. The purpose of this guide is to assist researchers facing similar demands in comprehending the steps to configure their servers. Additionally, it aims to address potential issues that may arise during the configuration process, along with their respective solutions.</p> <h2 id="create-a-new-user-with-root-privileges">Create a new user (with root privileges)</h2> <p>Typically, the server comes with a default user named after the vendor, in my instance, <code class="language-plaintext highlighter-rouge">inspur</code>. This user is a regular user but can gain root privileges by executing commands using the <code class="language-plaintext highlighter-rouge">sudo</code> prefix followed by typing its password. To create a custom account with similar privileges, follow these steps:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>useradd <span class="nt">-d</span> <span class="s2">"/home/&lt;user_name&gt;"</span> <span class="nt">-m</span> <span class="nt">-s</span> <span class="s2">"/bin/bash"</span> &lt;user_name&gt;
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">-d "/volume1/home/&lt;user_name&gt;"</code> will set <code class="language-plaintext highlighter-rouge">/volume1/home/&lt;user_name&gt;</code> as home directory of the new Ubuntu account.</li> <li><code class="language-plaintext highlighter-rouge">-m</code> will create the user’s home directory.</li> <li><code class="language-plaintext highlighter-rouge">-s "/bin/bash"</code>will set <code class="language-plaintext highlighter-rouge">/bin/bash</code> as login shell of the new account. This command will create a regular account <code class="language-plaintext highlighter-rouge">&lt;user_name&gt;</code>. If you want <code class="language-plaintext highlighter-rouge">&lt;user_name&gt;</code> to have root privileges, type:</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>useradd <span class="nt">-d</span> <span class="s2">"/home/&lt;user_name&gt;"</span> <span class="nt">-m</span> <span class="nt">-s</span> <span class="s2">"/bin/bash"</span> <span class="nt">-G</span> <span class="nb">sudo</span> &lt;user_name&gt;
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">-G sudo</code> ensures <code class="language-plaintext highlighter-rouge">&lt;user_name&gt;</code> to have admin access to the system.</li> </ul> <p>To set the password of the new account, conduct:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>passwd &lt;user_name&gt;
</code></pre></div></div> <p>After running the command, you will be prompted to type your password. Please note that Ubuntu will not display the password you are typing explicitly or implicitly (like dots). Just type the password you want to set and press <code class="language-plaintext highlighter-rouge">Enter</code>.</p> <h2 id="change-terminal-prompt-optional">Change terminal prompt (optional)</h2> <p>A beautiful terminal prompt can bring a beautiful day. To change the terminal prompt, conduct</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~
vim .bashrc
</code></pre></div></div> <p>You will see a paragraph like this:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code># uncomment for a colored prompt, if the terminal has the capability; turned
# off by default to not distract the user: the focus in a terminal window
# should be on the output of commands, not on the prompt
#force_color_prompt=yes
</code></pre></div></div> <p>Uncomment the last line <code class="language-plaintext highlighter-rouge">#force_color_prompt=yes</code>. Below this paragraph you will also see some codes:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$color_prompt</span><span class="s2">"</span> <span class="o">=</span> <span class="nb">yes</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nv">PS1</span><span class="o">=</span><span class="s1">'${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ '</span>
<span class="k">else
    </span><span class="nv">PS1</span><span class="o">=</span><span class="s1">'${debian_chroot:+($debian_chroot)}\u@\h:\w\$ '</span>
<span class="k">fi</span>
</code></pre></div></div> <p>Modify the first <code class="language-plaintext highlighter-rouge">PS1</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">PS1</span><span class="o">=</span><span class="s1">'\[\033[35m\]\t\[\033[m\]-\[\033[36m\]\u\[\033[m\]@\[\033[32m\]\h:\[\033[33;1m\]\w\[\033[m\]\$ '</span>
</code></pre></div></div> <p>This is my <code class="language-plaintext highlighter-rouge">PS1</code> value. Save the <code class="language-plaintext highlighter-rouge">.bashrc</code> file, close your current terminal and open a new one. The terminal prompt will look like this:</p> <p><span style="color:magenta">23:02:02</span>-<span style="color:skyblue">tdeng</span>@<span style="color:green">inspur-NP5570M5:</span><span style="color:gold">~/data</span>$ .</p> <h2 id="enable-remote-access">Enable remote access</h2> <p>If you wish to access the server from outside its physical location, you need to enable remote access. In my case, I connected the server to the campus network, allowing me to access it from any location within the campus. To enable remote access, you need to install the <code class="language-plaintext highlighter-rouge">openssh-server</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>openssh-server
</code></pre></div></div> <p>If the firewall <code class="language-plaintext highlighter-rouge">UFW</code> is enabled, make sure to open the SSH port:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>ufw allow ssh
</code></pre></div></div> <p>To test whether you can access the server from a Windows system:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>telnet &lt;remote_ip&gt; &lt;remote_port&gt;
</code></pre></div></div> <p><a href="https://linuxize.com/post/how-to-enable-ssh-on-ubuntu-20-04/">This website</a> might be useful.</p> <p>When you log in to the server using the newly created user with bash, you might encounter an error like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/bin/xauth: file /home/&lt;user_name&gt;/.Xauthority does not exist
</code></pre></div></div> <p>Solution:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chown</span> &lt;user_name&gt;:&lt;user_name&gt; <span class="nt">-R</span> /home/&lt;user_name&gt;
</code></pre></div></div> <h2 id="connect-to-github">Connect to GitHub</h2> <p>I suppose you already have a GitHub account. Install <code class="language-plaintext highlighter-rouge">git</code> first:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>git
git <span class="nt">--version</span>
</code></pre></div></div> <p>Then configure certain information about your GitHub account:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git config <span class="nt">--global</span> user.name <span class="s2">"&lt;github_account_name&gt;"</span>
git config <span class="nt">--global</span> user.email <span class="s2">"&lt;github_account_email&gt;"</span>
</code></pre></div></div> <p>Connect to GitHub:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-keygen <span class="nt">-C</span> <span class="s2">"&lt;github_account_email&gt;"</span> <span class="nt">-t</span> rsa  <span class="c"># default: just press Enter 3 times</span>
<span class="nb">cd</span> ~/.ssh
vim id_rsa.pub  <span class="c"># open the id_rsa.pub file</span>
</code></pre></div></div> <p>Finally, copy the text in <code class="language-plaintext highlighter-rouge">id_rsa.pub</code>, log in GitHub, and create an SSH key at <code class="language-plaintext highlighter-rouge">Settings</code> → <code class="language-plaintext highlighter-rouge">SSH and GPG keys</code> → <code class="language-plaintext highlighter-rouge">New SSH key</code>.</p> <p>Test the connection:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-T</span> git@github.com
</code></pre></div></div> <h2 id="configure-the-python-environment">Configure the python environment</h2> <h3 id="install-anaconda">Install Anaconda</h3> <p>Just follow the <a href="https://docs.anaconda.com/free/anaconda/install/linux/">official installation guide</a>. I prefer install anaconda at <code class="language-plaintext highlighter-rouge">/usr/local/anaconda3</code>. You don’t need to create this folder in advance. During the installation you will have chance to specify the installation directory.</p> <p>To initialize conda, conduct</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/path/to/conda init  <span class="c"># /usr/local/anaconda3/bin/conda in my case</span>
</code></pre></div></div> <p>and reopen the terminal.</p> <h3 id="createdelete-environments">Create/delete environments</h3> <p>I recommend creating new environments and installing site packages with root privileges (<code class="language-plaintext highlighter-rouge">sudo su</code>) to restrict regular users from modifying the environments. If a regular user wants to update an environment, they should contact the system administrator for assistance. If he/she doesn’t and conduct a command secretly like</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda update <span class="nt">--all</span>
</code></pre></div></div> <p>he/she will proceed with the update plan but finally fail with error info:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Proceed <span class="o">([</span>y]/n<span class="o">)</span>? y


Downloading and Extracting Packages

Preparing transaction: <span class="k">done
</span>Verifying transaction: failed

EnvironmentNotWritableError: The current user does not have write permissions to the target environment.
  environment location: /usr/local/anaconda3/envs/bio
  uid: 1001
  gid: 1001
</code></pre></div></div> <p>The command for creating new environment is:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">--name</span> &lt;new_env_name&gt; <span class="nv">python</span><span class="o">=</span>3.11 <span class="nt">--no-default-packages</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">--name &lt;new_env_name&gt;</code> will set the name of the new environment.</li> <li><code class="language-plaintext highlighter-rouge">python=3.11</code> means conda will install python 3.11 in the new environment.</li> <li><code class="language-plaintext highlighter-rouge">--no-default-packages</code> will only install python. No other site packages will be included.</li> </ul> <p>I did not modify the <code class="language-plaintext highlighter-rouge">base</code> environment and proceeded to create two new environments: <code class="language-plaintext highlighter-rouge">jupyter</code> and <code class="language-plaintext highlighter-rouge">bio</code>. <code class="language-plaintext highlighter-rouge">jupyter</code> only contains packages related to jupyterhub, while <code class="language-plaintext highlighter-rouge">bio</code> encompasses all the necessary packages for research purposes.</p> <p>If you wish to delete an environment for any reason, utilize the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda remove <span class="nt">--name</span> &lt;env_name&gt; <span class="nt">--all</span>
</code></pre></div></div> <h3 id="install-python-packages">Install python packages</h3> <h4 id="mamba">Mamba</h4> <p>Before install other python packages, I recommend to install <code class="language-plaintext highlighter-rouge">conda-libmamba-solver</code> first, which is a faster solver:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda update <span class="nt">-n</span> &lt;env_name&gt; conda
conda <span class="nb">install</span> <span class="nt">-n</span> &lt;env_name&gt; conda-libmamba-solver
conda config <span class="nt">--set</span> solver libmamba
</code></pre></div></div> <p>The plugin needs to be present in the same environment you use <code class="language-plaintext highlighter-rouge">conda</code> from; most of the time, this is your <code class="language-plaintext highlighter-rouge">base</code> environment.</p> <h4 id="jupyterhub">JupyterHub</h4> <p>You may want to use <code class="language-plaintext highlighter-rouge">JupyterHub</code>.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge jupyterhub jupyterlab notebook jupyter-lsp-python jupyterlab-lsp
</code></pre></div></div> <p>I recommend to install the <a href="https://github.com/jupyter-lsp/jupyterlab-lsp">jupyterlab-lsp</a>, a powerful coding assistance for JupyterLab. Another useful plugin is <a href="https://github.com/deshaw/jupyterlab-execute-time">jupyterlab-execute-time</a>, which can display cell timings in JupyterLab. Use the following command to install it:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge jupyterlab_execute_time
</code></pre></div></div> <p>Refer to <a href="https://jupyterhub.readthedocs.io/en/stable/tutorial/getting-started/config-basics.html">this website</a> for the configuration of JupyterHub.</p> <p>Refer to <a href="https://professorkazarinoff.github.io/jupyterhub-engr114/systemd/">this website</a> for how to run JupyterHub as a system service.</p> <p>Refer to <a href="https://linuxconfig.org/how-to-start-service-on-boot-on-ubuntu-20-04">this website</a> for how to start the service on boot.</p> <h4 id="adddelete-an-environment-as-a-kernel">Add/delete an environment as a kernel</h4> <p>To add an environment as a kernel:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda activate &lt;env_name&gt;
conda <span class="nb">install </span>ipykernel  <span class="c"># if the env doesn't contain this package</span>
python <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--name</span> &lt;kernel_name&gt;
</code></pre></div></div> <p>These commands add <code class="language-plaintext highlighter-rouge">&lt;env_name&gt;</code> environment as a kernel with name <code class="language-plaintext highlighter-rouge">&lt;kernel_name&gt;</code>. If your Python is 3.11, you may need to modify the last command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-Xfrozen_modules</span><span class="o">=</span>off <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--name</span> bio
</code></pre></div></div> <p>To delete a kernel:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter kernelspec list
jupyter kernelspec uninstall &lt;kernel_name&gt;
</code></pre></div></div> <h4 id="other-packages">Other packages</h4> <p>Our research involves deep learning, so I need to install <code class="language-plaintext highlighter-rouge">pytorch</code> and <code class="language-plaintext highlighter-rouge">tensorflow</code> along with other required packages:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-c</span> pytorch <span class="nt">-c</span> nvidia <span class="nt">-c</span> conda-forge scvi-tools tensorflow torchvision torchaudio
conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge scanpy squidpy ipykernel ipywidgets rpy2 opencv biopython
conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge xgboost lightgbm catboost
</code></pre></div></div> <p>Note: <code class="language-plaintext highlighter-rouge">pytorch</code> and <code class="language-plaintext highlighter-rouge">pytorch-lightning</code> are dependencies of <code class="language-plaintext highlighter-rouge">scvi-tools</code> so you don’t need to install these two packages again.</p> <p>Sometimes you may use <code class="language-plaintext highlighter-rouge">conda search &lt;package_name&gt;</code> to search for a package with a specific build number. To install a specific version/build of a certain packages, conduct:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> &lt;package_name&gt;<span class="o">=</span>&lt;version&gt;<span class="o">=</span>&lt;build_string&gt;
</code></pre></div></div> <h4 id="check-pytorchtensorflow">Check pytorch/tensorflow</h4> <p>If you are also a user of <code class="language-plaintext highlighter-rouge">pytorch</code> or <code class="language-plaintext highlighter-rouge">tensorflow</code> and you have one or more available GPU(s), you can execute the following codes to verify whether the GPU(s) can be recognized and utilized by the respective deep learning frameworks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># check pytorch
</span><span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device_count</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">current_device</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># check tensorflow
</span><span class="nf">print</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">list_physical_devices</span><span class="p">(</span><span class="sh">'</span><span class="s">GPU</span><span class="sh">'</span><span class="p">))</span>
</code></pre></div></div> <p>Here I also provide a script to ensure that <code class="language-plaintext highlighter-rouge">pytorch</code> can use the GPU(s) to train and test neural networks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torchvision</span>
<span class="kn">import</span> <span class="n">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>


<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="c1"># define image preprocessing
</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Pad</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomHorizontalFlip</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomCrop</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">()])</span>

<span class="c1"># download the CIFAR-10 dataset
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nc">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">data/</span><span class="sh">'</span><span class="p">,</span>
                                             <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                                             <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                                             <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nc">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">data/</span><span class="sh">'</span><span class="p">,</span>
                                            <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                                            <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">())</span>

<span class="c1"># load data
</span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                           <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>            <span class="c1"># number of subprocesses to use for data loading
</span>                                           <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>          <span class="c1"># the data loader will copy Tensors into CUDA pinned memory before returning them
</span>                                           <span class="n">prefetch_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>        <span class="c1"># number of batches loaded in advance by each worker
</span>                                           <span class="n">persistent_workers</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># the data loader will not shutdown the worker processes after a dataset has been consumed once
</span>                                           <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                          <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                          <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                                          <span class="n">prefetch_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> 
                                          <span class="n">persistent_workers</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># 3x3 convolution kernel
</span><span class="k">def</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
                     <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># define the residual block
</span><span class="k">class</span> <span class="nc">ResidualBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">downsample</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="n">downsample</span>
  
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">downsample</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="n">residual</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># define the structure of ResNet
</span><span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="mi">16</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">avg_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">AvgPool2d</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
  
    <span class="k">def</span> <span class="nf">make_layer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">):</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="nf">conv3x3</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">))</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">block</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">block</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
  
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">avg_pool</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
  

<span class="n">model</span> <span class="o">=</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model = nn.DataParallel(model)  # uncomment this line if you have multiple GPUs
</span>

<span class="c1"># define loss function
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># function for the update of learning rate
</span><span class="k">def</span> <span class="nf">update_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>  
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

<span class="c1"># train the ResNet
</span><span class="n">total_step</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">curr_lr</span> <span class="o">=</span> <span class="n">learning_rate</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  
        <span class="c1"># forward step
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
  
        <span class="c1"># backward step
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
  
        <span class="c1"># report every 10 steps
</span>        <span class="nf">if </span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print </span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}</span><span class="sh">"</span>
                   <span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">total_step</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()))</span>

    <span class="c1"># update learning rate
</span>    <span class="nf">if </span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">curr_lr</span> <span class="o">/=</span> <span class="mi">3</span>
        <span class="nf">update_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">curr_lr</span><span class="p">)</span>

<span class="c1"># test the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy of the model on the test images: {} %</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">))</span>
</code></pre></div></div> <p>Note that if you have multiple GPUs, you need to uncomment the line below the code which creates the model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># model = nn.DataParallel(model)
</span></code></pre></div></div> <p>You can use this command to monitor the GPU(s) during training:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch <span class="nt">-n</span> 0.2 nvidia-smi
</code></pre></div></div> <h2 id="configure-the-r-environment">Configure the R environment</h2> <h3 id="install-r">Install R</h3> <p>Refer to <a href="https://www.digitalocean.com/community/tutorials/how-to-install-r-on-ubuntu-22-04">this website</a> for instructions on adding the external repository maintained by CRAN for <code class="language-plaintext highlighter-rouge">APT</code> and subsequently installing <code class="language-plaintext highlighter-rouge">R</code>.</p> <h3 id="install-rstudio">Install RStudio</h3> <p>Follow the <a href="https://posit.co/download/rstudio-server/">official installation guide</a>. This should be easier than installing <code class="language-plaintext highlighter-rouge">JupyterHub</code>.</p> <h3 id="install-r-packages">Install R packages</h3> <p>As an example, let’s install one of the most famous R package in the field of single-cell genomics, <a href="https://satijalab.org/seurat/index.html"><code class="language-plaintext highlighter-rouge">Seurat</code></a>. Before the installation, you need to install some system-level dependencies first:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>cmake pandoc pandoc-citeproc libcurl4-openssl-dev libfontconfig1-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev imagemagick libmagick++-dev libhdf5-dev libgsl-dev libssl-dev
</code></pre></div></div> <p>Then the process of installing <code class="language-plaintext highlighter-rouge">Seurat</code> should be very smooth:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chooseCRANmirror</span><span class="p">(</span><span class="n">graphics</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"devtools"</span><span class="p">)</span><span class="w">
</span><span class="n">remotes</span><span class="o">::</span><span class="n">install_github</span><span class="p">(</span><span class="s2">"satijalab/seurat"</span><span class="p">,</span><span class="w"> </span><span class="s2">"seurat5"</span><span class="p">,</span><span class="w"> </span><span class="n">quiet</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p>If you intend to install an extremely large R package, you’d better set a longer timeout:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">options</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="m">999</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"&lt;large_package&gt;"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p>When running <code class="language-plaintext highlighter-rouge">devtools::install_github()</code>, you may encounter an error complaining that the API rate limit has been exceeded. The solution to this issue is to create a GitHub token.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">usethis</span><span class="o">::</span><span class="n">create_github_token</span><span class="p">()</span><span class="w">
</span></code></pre></div></div> <p>Run this code and log in to your GitHub account. Click <code class="language-plaintext highlighter-rouge">Settings</code> → <code class="language-plaintext highlighter-rouge">Developer settings</code> → <code class="language-plaintext highlighter-rouge">Personal access token</code> → <code class="language-plaintext highlighter-rouge">Tokens (classic)</code> and generate a token. Run</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">usethis</span><span class="o">::</span><span class="n">edit_r_environ</span><span class="p">()</span><span class="w">
</span></code></pre></div></div> <p>to open the <code class="language-plaintext highlighter-rouge">.Renviron</code> file, in which type</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GITHUB_PAT=&lt;your_personal_key&gt;
</code></pre></div></div> <p>and save the file. Quit the current R session and start a new session. The error should be solved.</p> <h2 id="synchronize-data">Synchronize data</h2> <p>Refer to <a href="https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories">this website</a> for detailed instructions on how to synchronize data stored on another server.</p> <p>The key command is</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rsync <span class="nt">-r</span> /path/to/sync/ &lt;username&gt;@&lt;remote_host&gt;:&lt;destination_directory&gt;
</code></pre></div></div> <p>which “pushes” a directory from the system you are logging in to another system.</p> <p>If you are synchronizing a large file, you may want to monitor the process:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch <span class="nt">-n</span> &lt;time_interval&gt; <span class="nb">du</span> <span class="nt">-sh</span> /path/to/large/file
</code></pre></div></div> <p>Now, your server should be well-suited for your bioinformatics research. Enjoy it!</p>]]></content><author><name></name></author><category term="computer"/><category term="deployment"/><category term="server"/><category term="Ubuntu"/><summary type="html"><![CDATA[how to deploy a server for bioinformatics research]]></summary></entry></feed>