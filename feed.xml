<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://torydeng.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://torydeng.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-07T15:47:37+00:00</updated><id>https://torydeng.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Perron-Frobenius Theorem</title><link href="https://torydeng.github.io/blog/2023/perron-frobenius/" rel="alternate" type="text/html" title="The Perron-Frobenius Theorem"/><published>2023-12-07T00:00:00+00:00</published><updated>2023-12-07T00:00:00+00:00</updated><id>https://torydeng.github.io/blog/2023/perron-frobenius</id><content type="html" xml:base="https://torydeng.github.io/blog/2023/perron-frobenius/"><![CDATA[<p>The <a href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem">Perron-Frobenius Theorem</a> establishes powerful assertions about the eigenvalues and eigenvectors of certain types of matrices that are non-negative, which are incredibly insightful when dealing with dynamical systems, economics, demography, and beyond. This post offers an accessible proof of the theorem, which is carefully curated from lecture contents of CIE6002 Matrix Analysis.</p> <h2 id="notations">Notations</h2> <ul> <li>\(\boldsymbol{A} \in \mathbb{R}^{m \times m}\): an \(m\) by \(m\) matrix.</li> <li>\(\Vert \cdot \Vert\): in most cases it refers to matrix norm.</li> <li>\(\displaystyle \Vert \boldsymbol{A} \Vert_1 = \max_{j=1,2,\ldots,m} \sum_{i=1}^{m} \boldsymbol{A}_{ij}\): the maximum absolute column sum of the matrix.</li> <li>\(\displaystyle \Vert \boldsymbol{A} \Vert_\infty = \max_{i=1,2,\ldots,m} \sum_{j=1}^{m} \boldsymbol{A}_{ij}\): the maximum absolute row sum of the matrix.</li> </ul> <h2 id="the-theorem">The theorem</h2> <p>Let \(\boldsymbol{A} \in \mathbb{R}^{n \times n}\) be positive. That is, \(\boldsymbol{A}_{ij} &gt; 0, \forall 1 \le i,j \le m\). The <em>spectral radius</em> is defined as</p> \[\rho(\boldsymbol{A}) = \max_{i} |\lambda_i|\] <p>where \(\lambda_i\) is the \(i\)-th eigenvalue of \(\boldsymbol{A}\). Then</p> <ol> <li>\(\rho(\boldsymbol{A}) &gt; 0\), and \(\rho(\boldsymbol{A})\) is an eigenvalue of \(\boldsymbol{A}\);</li> <li>The corresponding eigenvector of \(\rho(\boldsymbol{A})\) is positive (or negative);</li> <li>\(\lvert \lambda \rvert &lt; \rho(\boldsymbol{A})\) for any \(\boldsymbol{A}\)’s eigenvalue \(\lambda \ne \rho(\boldsymbol{A})\);</li> <li>\(\operatorname{dim}(\operatorname{Null}(\boldsymbol{A} - \rho(\boldsymbol{A})\boldsymbol{I})) = 1\).</li> </ol> <h2 id="some-lemmas">Some lemmas</h2> <p><strong>Lemma 1:</strong> \(\rho(\boldsymbol{A}) \le \Vert \boldsymbol{A} \Vert\) for any matrix norm \(\Vert\cdot\Vert.\)</p> <p><strong>Proof:</strong> Let \(\boldsymbol{Av} = \lambda \boldsymbol{v}\) with \(\lvert\lambda\rvert = \rho(\boldsymbol{A})\). Let \(\boldsymbol{V} = \boldsymbol{v} \boldsymbol{1}^\top \in \mathbb{R}^{m \times m}\). Then</p> \[\begin{align*} &amp;\boldsymbol{AV} = \lambda \boldsymbol{V} \\ \Rightarrow &amp;\Vert\boldsymbol{AV}\Vert = \lvert \lambda \rvert \cdot \Vert\boldsymbol{V}\Vert \le \Vert\boldsymbol{A}\Vert \cdot \Vert\boldsymbol{V}\Vert \\ \Rightarrow &amp; \lvert \lambda \rvert = \rho(\boldsymbol{A}) \le \Vert\boldsymbol{A}\Vert. \end{align*}\] <hr/> <p><strong>Lemma 2:</strong> Given \(\varepsilon &gt; 0\). There exists a matrix norm \(\Vert\cdot\Vert\) s.t. \(\rho(\boldsymbol{A}) \le \Vert\boldsymbol{A}\Vert \le \rho(\boldsymbol{A}) + \varepsilon \Rightarrow \rho(\boldsymbol{A}) = \inf_{\Vert\cdot\Vert} \Vert\boldsymbol{A}\Vert\).</p> <p><strong>Proof:</strong> The Schur triangularization of \(\boldsymbol{A}\) is \(\boldsymbol{A} = \boldsymbol{UTU}^\top\), where \(\boldsymbol{U}\) is unitary and diagonals \(\lambda_1, \ldots, \lambda_m\) of \(\boldsymbol{T}\) are eigenvalues of \(\boldsymbol{A}\). Define</p> \[\begin{align*} \Vert\boldsymbol{A}\Vert &amp;\triangleq \Vert(\boldsymbol{UD}_t^{-1})^{-1}\boldsymbol{A}(\boldsymbol{UD}_t^{-1})\Vert_1 \\ &amp;= \Vert\boldsymbol{D}_t \boldsymbol{U}^\top \boldsymbol{AUD}_t^{-1}\Vert_1 \\ &amp;= \Vert\boldsymbol{D}_t \boldsymbol{T} \boldsymbol{D}_t^{-1}\Vert_1 \\ &amp;= \Vert\begin{bmatrix} \lambda_1 &amp; t^{-1}T_{12} &amp; t^{-2}T_{13} &amp; \cdots &amp; t^{-m+1}T_{1m}\\ &amp; \lambda_2 &amp; t^{-1}T_{13} &amp; \cdots &amp; t^{-m+2}T_{2m}\\ &amp; &amp; \lambda_3 &amp; \cdots &amp; t^{-m+3}T_{3m} \\ &amp; &amp; &amp; \ddots &amp; \vdots \\ &amp; &amp; &amp; &amp; \lambda_m \end{bmatrix}\Vert_1 \\ &amp;\le \rho(\boldsymbol{A}) + \varepsilon \quad \text{for large}\ t \end{align*}\] <p>where \(\boldsymbol{D}_t = \begin{bmatrix} t^1 &amp; &amp; &amp; \\ &amp; t^2 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; t^m \end{bmatrix}.\)</p> <hr/> <p><strong>Lemma 3:</strong> \(\lim_{k \to \infty} \boldsymbol{A}^k = \boldsymbol{0} \iff \rho(\boldsymbol{A}) &lt; 1.\)</p> <p><strong>Proof:</strong> The “\(\Rightarrow\)” part. Let \(\boldsymbol{Av} = \lambda \boldsymbol{v}\) for any eigenvalue \(\lambda\) of \(\boldsymbol{A}\). Then</p> \[\boldsymbol{A}^k\boldsymbol{v} = \lambda^k \boldsymbol{v} \Rightarrow \lambda^k \to 0 \Rightarrow |\lambda| &lt; 1 \Rightarrow \rho(\boldsymbol{A}) &lt; 1.\] <p>The “\(\Leftarrow\)” part. By Lemma 2, \(\exists\ \Vert\cdot\Vert\) s.t. \(\Vert\boldsymbol{A}\Vert &lt; 1.\) Then</p> \[0 \le \lim_{k \to \infty} \Vert\boldsymbol{A}^k\Vert \le \lim_{k \to \infty} \Vert\boldsymbol{A}\Vert^k = 0 \Rightarrow \lim_{k \to \infty} \boldsymbol{A}^k = \boldsymbol{0}.\] <hr/> <p><strong>Lemma 4:</strong> Let \(\boldsymbol{A}, \boldsymbol{B} \in \mathbb{R}^{m \times m}\) and \(\vert \boldsymbol{A} \vert \le \boldsymbol{B}\) element-wise. Then</p> \[\rho(\boldsymbol{A}) \le \rho(\boldsymbol{\vert A \vert}) \le \rho(\boldsymbol{B}).\] <p><strong>Proof:</strong></p> \[\begin{align*} &amp;\boldsymbol{A} \le \boldsymbol{\vert A \vert} \le \boldsymbol{B} \\ \Rightarrow&amp; \boldsymbol{A}^k \le \boldsymbol{\vert A \vert}^k \le \boldsymbol{B}^k \\ \Rightarrow&amp; \Vert \boldsymbol{A}^k \Vert_F \le \Vert \boldsymbol{\vert A \vert}^k \Vert_F \le \Vert \boldsymbol{B}^k \Vert_F \\ \Rightarrow&amp; \Vert \boldsymbol{A}^k \Vert_F^{1/k} \le \Vert \boldsymbol{\vert A \vert}^k \Vert_F^{1/k} \le \Vert \boldsymbol{B}^k \Vert_F^{1/k} \\ \Rightarrow&amp; \rho(\boldsymbol{A}) \le \rho(\boldsymbol{\vert A \vert}) \le \rho(\boldsymbol{B}). \end{align*}\] <p>The last step is given by Theorem 1.</p> <p><strong>Corollary 4.1:</strong> Let \(\boldsymbol{A} \ge \boldsymbol{0}\) element-wise. Then for any principal submatrix of \(\boldsymbol{A}\), denoted as \(\tilde{\boldsymbol{A}}\), we have \(\rho(\tilde{\boldsymbol{A}}) \le \rho(\boldsymbol{A}) \Rightarrow \max_{i} \boldsymbol{A}_{ii} \le \rho(\boldsymbol{A})\).</p> <hr/> <p><strong>Lemma 5:</strong> Let \(\boldsymbol{A} \ge \boldsymbol{0}\) element-wise. If row (column) sums of \(\boldsymbol{A}\) are constant, then \(\rho(\boldsymbol{A}) = \Vert \boldsymbol{A} \Vert_\infty\) (\(\rho(\boldsymbol{A}) = \Vert \boldsymbol{A} \Vert_1\)).</p> <p><strong>Proof:</strong> Suppose \(\boldsymbol{A1} = \alpha \boldsymbol{1}\) where \(\alpha \ge 0\) is the row sum, and thus is an eigenvalue of \(\boldsymbol{A}\). So \(\alpha \le \rho(\boldsymbol{A})\). But \(\alpha = \Vert \boldsymbol{A} \Vert_\infty \ge \rho(\boldsymbol{A})\), so \(\rho(\boldsymbol{A}) = \Vert \boldsymbol{A} \Vert_\infty.\) The column sum case is similar.</p> <hr/> <p><strong>Lemma 6:</strong> Let \(\boldsymbol{A} \ge \boldsymbol{0}\) element-wise. Then</p> \[\min_{i = 1, \ldots, m} \sum_{j = 1}^m \boldsymbol{A}_{ij} \le \rho(\boldsymbol{A}) \le \Vert \boldsymbol{A} \Vert_\infty,\ \min_{j = 1, \ldots, m} \sum_{i = 1}^m \boldsymbol{A}_{ij} \le \rho(\boldsymbol{A}) \le \Vert \boldsymbol{A} \Vert_1\] <p><strong>Proof:</strong> Let</p> \[\alpha = \min_{i = 1, \ldots, m} \sum_{j = 1}^m \boldsymbol{A}_{ij} \ne 0.\] <p>\(\alpha = 0\) is a trivial case, so assume \(\alpha \ne 0\). Construct a new matrix \(\boldsymbol{B}\) by multiplying \(\alpha / \sum_{j=1}^m \boldsymbol{A}_{ij}\) to each \(i\)-th row of \(\boldsymbol{A}\).</p> <p>So \(\boldsymbol{0} \le \boldsymbol{B} \le \boldsymbol{A}\) element-wise, and \(\boldsymbol{B}\) has a constant row sum equal to \(\alpha \Rightarrow \alpha = \rho(\boldsymbol{B}) \le \rho(\boldsymbol{A})\) by Lemma 4 and 5. \(\rho(\boldsymbol{A}) \le \Vert \boldsymbol{A} \Vert_\infty\) by Lemma 1. The column sum case is similar.</p> <hr/> <p><strong>Lemma 7:</strong> Let \(\boldsymbol{A} &gt; \boldsymbol{0}\) element-wise. Suppose \(\boldsymbol{Ax} = \lambda \boldsymbol{x}\), and \(\vert \lambda \vert = \rho(\boldsymbol{A})\). Then \(\exists\ \theta \in \mathbb{R}\) s.t. \(e^{-j\theta} \boldsymbol{x} = \vert \boldsymbol{x} \vert &gt; \boldsymbol{0}\) element-wise. Here \(j = \sqrt{-1}\).</p> <p><strong>Proof:</strong> \(\vert \boldsymbol{Ax} \vert = \vert \lambda \vert \vert \boldsymbol{x} \vert = \rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert\). By Theorem 3, \(\boldsymbol{A} \vert \boldsymbol{x} \vert = \rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert\). So \(\vert \boldsymbol{Ax} \vert = \boldsymbol{A} \vert \boldsymbol{x} \vert\) and \(\vert \boldsymbol{x} \vert &gt; \boldsymbol{0}\). For any \(1 \le i \le m\),</p> \[[\vert \boldsymbol{Ax} \vert]_i = \left\vert \sum_{j=1}^m \boldsymbol{A}_{ik} \boldsymbol{x}_k \right\vert = \sum_{k=1}^m \boldsymbol{A}_{ik} \vert \boldsymbol{x}_k \vert\] <p>For any \(\boldsymbol{x}_k \in \mathbb{C}, \boldsymbol{x}_k = \vert \boldsymbol{x}_k \vert e^{j\theta_k} = \vert \boldsymbol{x}_k \vert \cos(\theta_k) + \vert \boldsymbol{x}_k \vert \sin(\theta_k) \cdot j\). So</p> \[\begin{align*} &amp;\left\vert \sum_{j=1}^m \boldsymbol{A}_{ik} \boldsymbol{x}_k \right\vert = \sum_{k=1}^m \boldsymbol{A}_{ik} \vert \boldsymbol{x}_k \vert \\ \Rightarrow&amp; \left(\sum_{j=1}^m \boldsymbol{A}_{ik} \boldsymbol{x}_k\right) e^{-j\theta} = \sum_{k=1}^m \boldsymbol{A}_{ik} \boldsymbol{x}_k e^{-j\theta_k} \\ \Rightarrow&amp; \sum_{j=1}^m \boldsymbol{A}_{ik} \boldsymbol{x}_k e^{-j\theta} = \sum_{k=1}^m \boldsymbol{A}_{ik} \boldsymbol{x}_k e^{-j\theta_k} \\ \Rightarrow&amp; \theta_k = \theta, k=1, \ldots, m. \end{align*}\] <h2 id="some-theorems">Some theorems</h2> <p><strong>Theorem 1:</strong> \(\rho(\boldsymbol{A}) = \lim_{k \to \infty} \Vert \boldsymbol{A}^k \Vert^{1/k}\) for any matrix norm.</p> <p><strong>Proof:</strong> The aim is to show \(0 \le \Vert \boldsymbol{A}^k \Vert^{1/k} - \rho(\boldsymbol{A}) \le \varepsilon\) for large \(k\).</p> <p>To see this, \(\rho^k(\boldsymbol{A}) = \rho(\boldsymbol{A^k}) \le \Vert \boldsymbol{A}^k \Vert\) by Lemma 2. Thus \(\rho(\boldsymbol{A}) \le \Vert \boldsymbol{A}^k \Vert^{1/k} \Rightarrow 0 \le \Vert \boldsymbol{A}^k \Vert^{1/k} - \rho(\boldsymbol{A})\).</p> <p>Let \(\tilde{\boldsymbol{A}} = \frac{1}{\varepsilon + \rho(\boldsymbol{A})} \boldsymbol{A}\). It’s easy to see \(\rho(\tilde{\boldsymbol{A}}) &lt; 1\). Then for \(k\) large enough \(\Vert \tilde{\boldsymbol{A}}^k \Vert \le 1 \Leftrightarrow \frac{1}{(\varepsilon + \rho(\boldsymbol{A}))^k} \Vert \boldsymbol{A}^k \Vert \le 1 \Leftrightarrow \Vert \boldsymbol{A}^k \Vert^{1/k} \le \varepsilon + \rho(\boldsymbol{A}).\)</p> <hr/> <p><strong>Theorem 2:</strong> For any \(\boldsymbol{A} \ge \boldsymbol{0}\) element-wise and \(\boldsymbol{x} &gt; \boldsymbol{0}\) element-wise,</p> \[\min_{i = 1, \ldots, m} \frac{1}{\boldsymbol{x}_i}\sum_{j=1}^m \boldsymbol{A}_{ij} \boldsymbol{x}_j \le \rho(\boldsymbol{A}) \le \max_{i = 1, \ldots, m} \frac{1}{\boldsymbol{x}_i}\sum_{j=1}^m \boldsymbol{A}_{ij} \boldsymbol{x}_j\] <p><strong>Proof:</strong> Define \(\bar{\boldsymbol{A}} \triangleq \boldsymbol{S}^{-1}\boldsymbol{A}\boldsymbol{S}\), where</p> \[\boldsymbol{S} = \begin{bmatrix} \boldsymbol{x}_1 &amp; &amp; &amp; \\ &amp; \boldsymbol{x}_2 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; \boldsymbol{x}_m \end{bmatrix},\ \boldsymbol{S}^{-1} = \begin{bmatrix} \frac{1}{\boldsymbol{x}_1} &amp; &amp; &amp; \\ &amp; \frac{1}{\boldsymbol{x}_2} &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; \frac{1}{\boldsymbol{x}_m} \end{bmatrix}.\] <p>\(\rho(\bar{\boldsymbol{A}}) = \rho(\boldsymbol{A})\) as \(\bar{\boldsymbol{A}}\) and \(\boldsymbol{A}\) have same eigenvalues. Apply Lemma 6 to \(\bar{\boldsymbol{A}}\).</p> <p><strong>Corollary 2.1:</strong> For any \(\boldsymbol{A} \ge \boldsymbol{0}\) element-wise and \(\boldsymbol{x} &gt; \boldsymbol{0}\) element-wise, if</p> \[\alpha \boldsymbol{x} \le \boldsymbol{Ax} \le \beta \boldsymbol{x}\] <p>where \(\alpha, \beta \ge 0\). Then \(\alpha \le \rho(\boldsymbol{A}) \le \beta\). If the inequality is strict, \(\alpha &lt; \rho(\boldsymbol{A}) &lt; \beta\).</p> <p><strong>Proof:</strong></p> \[\begin{align*} \alpha \boldsymbol{x}_i \le \sum_{j=1}^m \boldsymbol{A}_{ij}\boldsymbol{x}_j \Rightarrow \alpha \le \frac{1}{\boldsymbol{x}_i} \sum_{j=1}^m \boldsymbol{A}_{ij}\boldsymbol{x}_j \Rightarrow \alpha \le \min_{i = 1, \ldots, m} \frac{1}{\boldsymbol{x}_i}\sum_{j=1}^m \boldsymbol{A}_{ij} \boldsymbol{x}_j \le \rho(\boldsymbol{A}). \end{align*}\] <p>\(\rho(\boldsymbol{A}) \le \beta\) is similar.</p> <p><strong>Corollary 2.2:</strong> If \(\boldsymbol{A} \ge \boldsymbol{0}\) element-wise and has a eigenvector \(\boldsymbol{x} &gt; \boldsymbol{0}\) element-wise. Then the associated eigenvalue must be \(\rho(\boldsymbol{A})\).</p> <p><strong>Proof:</strong> \(\lambda \boldsymbol{x} \le \boldsymbol{Ax} \le \lambda \boldsymbol{x} \Rightarrow \lambda \le \rho(\boldsymbol{A}) \le \lambda \Rightarrow \rho(\boldsymbol{A}) = \lambda.\)</p> <hr/> <p><strong>Theorem 3:</strong> Let \(\boldsymbol{A} &gt; \boldsymbol{0}\) element-wise. Suppose \(\boldsymbol{Ax} = \lambda \boldsymbol{x}\) for some \(\lambda \in \mathbb{R}, \boldsymbol{x} \in \mathbb{R}^{m}\), and \(\vert \lambda \vert = \rho(\boldsymbol{A})\). Then \(\boldsymbol{A} \vert \boldsymbol{x} \vert = \rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert\) and \(\vert \boldsymbol{x} \vert &gt; \boldsymbol{0}\).</p> <p><strong>Proof:</strong> By Corollary 4.1, \(0 &lt; \max_i \boldsymbol{A}_{ii} \le \rho(\boldsymbol{A})\). \(\boldsymbol{Ax} = \lambda\boldsymbol{x} \Rightarrow \vert \boldsymbol{Ax} \vert = \vert \lambda \vert \vert \boldsymbol{x} \vert = \rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert \le \boldsymbol{A} \vert \boldsymbol{x} \vert.\) Define \(\boldsymbol{y} \triangleq \boldsymbol{A} \vert \boldsymbol{x} \vert - \rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert \ge \boldsymbol{0}\) element-wise.</p> <ul> <li>If \(\boldsymbol{y} \equiv \boldsymbol{0}\), i.e., \(\rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert = \boldsymbol{A} \vert \boldsymbol{x} \vert\) element-wise: since \(\boldsymbol{A} \vert \boldsymbol{x} \vert &gt; \boldsymbol{0}\) element-wise and \(0 &lt; \rho(\boldsymbol{A})\), \(\vert \boldsymbol{x} \vert &gt; \boldsymbol{0}\) element-wise.</li> <li>If \(\boldsymbol{y}_i &gt; 0\) for some \(i\), let \(\boldsymbol{z} \triangleq \boldsymbol{A} \vert \boldsymbol{x} \vert &gt; \boldsymbol{0}\) element-wise. Then</li> </ul> \[\begin{align*} &amp;\boldsymbol{0} &lt; \boldsymbol{Ay} = \boldsymbol{A}(\boldsymbol{A} \vert \boldsymbol{x} \vert - \rho(\boldsymbol{A}) \vert \boldsymbol{x} \vert) = \boldsymbol{Az} - \rho(\boldsymbol{A}) \boldsymbol{z} \\ \Rightarrow&amp; \rho(\boldsymbol{A}) \boldsymbol{z} &lt; \boldsymbol{Az} \\ \Rightarrow&amp; \rho(\boldsymbol{A}) &lt; \rho(\boldsymbol{A})\qquad \text{by Corollary 2.1} \end{align*}\] <p>which is a contradiction. So \(\boldsymbol{y} \equiv \boldsymbol{0}\).</p> <hr/> <p><strong>Theorem 4:</strong> Let \(\boldsymbol{A} &gt; \boldsymbol{0}\) element-wise. Then \(\forall\ \lambda \ne \rho(\boldsymbol{A}), \vert \lambda \vert &lt; \rho(\boldsymbol{A})\).</p> <p><strong>Proof:</strong> Suppose \(\exists\ \lambda \ne \rho(\boldsymbol{A})\) but \(\vert \lambda \vert = \rho(\boldsymbol{A})\) and \(\boldsymbol{Ax} = \lambda \boldsymbol{x}\). By Lemma 7, \(\exists\ \theta \in \mathbb{R}\) s.t. \(\vert \boldsymbol{x} \vert = e^{-j\theta} \boldsymbol{x} &gt; \boldsymbol{0}\). \(\boldsymbol{Ax} = \lambda \boldsymbol{x} \Rightarrow \boldsymbol{A} \vert \boldsymbol{x} \vert = \lambda \vert \boldsymbol{x} \vert\). By Corollary 2.2 \(\lambda = \rho(\boldsymbol{A})\), which is a contradiction.</p> <hr/> <p><strong>Theorem 5:</strong> Let \(\boldsymbol{A} &gt; \boldsymbol{0}\) element-wise. Suppose for two vectors \(\boldsymbol{w}, \boldsymbol{z}\) s.t. \(\boldsymbol{Aw} = \rho(\boldsymbol{A})\boldsymbol{w}, \boldsymbol{Az} = \rho(\boldsymbol{A})\boldsymbol{z}\). Then \(\exists\ \alpha \in \mathbb{C}\) s.t. \(\boldsymbol{w} = \alpha \boldsymbol{z}\), i.e., \(\operatorname{dim}(\operatorname{Null}(\boldsymbol{A} - \rho(\boldsymbol{A})\boldsymbol{I})) = 1\).</p> <p><strong>Proof:</strong> By Lemma 7, \(\exists\ \theta_1, \theta_2\) s.t. \(\boldsymbol{q} = \vert \boldsymbol{w} \vert = e^{-j\theta_1} \boldsymbol{w} &gt; \boldsymbol{0}, \boldsymbol{p} = \vert \boldsymbol{z} \vert = e^{-j\theta_2} \boldsymbol{z} &gt; \boldsymbol{0}\). By Theorem 3 \(\boldsymbol{Aq} = \rho(\boldsymbol{A})\boldsymbol{q}, \boldsymbol{Ap} = \rho(\boldsymbol{A})\boldsymbol{p}\). Let \(\beta = \min_{i=1,\ldots, m} \frac{\boldsymbol{q}_i}{\boldsymbol{p}_i}\) and \(\boldsymbol{r} = \boldsymbol{q} - \beta \boldsymbol{p} \ge \boldsymbol{0}\) with \(\boldsymbol{r}_j = 0\) for some \(j\). Then</p> \[\begin{align*} \boldsymbol{Ar} &amp;= \boldsymbol{Aq} - \beta \boldsymbol{Ap}\\ &amp;= \rho(\boldsymbol{A})\boldsymbol{q} - \beta \rho(\boldsymbol{A})\boldsymbol{p}\\ &amp;= \rho(\boldsymbol{A})\boldsymbol{r} \end{align*}\] <ul> <li>If \(\boldsymbol{r} \equiv \boldsymbol{0}\), then \(\boldsymbol{q} = \beta \boldsymbol{p}\).</li> <li>If \(\boldsymbol{r}_k &gt; 0\) for some \(k\), then \(\boldsymbol{Ar} = \rho(\boldsymbol{A})\boldsymbol{r}&gt;\boldsymbol{0} \Rightarrow \boldsymbol{r} &gt; \boldsymbol{0}\) which is a contradiction.</li> </ul> <p>So \(\boldsymbol{r} = \boldsymbol{0} \Rightarrow \boldsymbol{q} = \beta \boldsymbol{p} \Rightarrow \boldsymbol{w} = \alpha \boldsymbol{z}\).</p> <h2 id="an-application">An application</h2> <p><strong>Irreducible matrix:</strong> Let \(\boldsymbol{A} \in \mathbb{R}^{m \times m}, \boldsymbol{A} \ge \boldsymbol{0}\) element-wise. \(\boldsymbol{A}\) is <em>irreducible</em> if for each index \((i, j), \exists\ k \in \mathbb{N}^+\) s.t. \([\boldsymbol{A}^k]_{ij} &gt; 0\).</p> <p><strong>Subinvariance Theorem:</strong> Let \(\boldsymbol{A} \ge \boldsymbol{0}\) be irreducible. Suppose for some \(\boldsymbol{y} \ge \boldsymbol{0}, \boldsymbol{y} \ne \boldsymbol{0}\) and \(s &gt; 0\), we have \(\boldsymbol{Ay} \le s\boldsymbol{y}\). Then</p> <ol> <li>\(\boldsymbol{y} &gt; \boldsymbol{0}\);</li> <li>\(\rho(\boldsymbol{A}) \le s\);</li> <li>\(\rho(\boldsymbol{A}) = s \iff \boldsymbol{Ay} = s\boldsymbol{y}\).</li> </ol> <p><strong>Proof:</strong></p> <ol> <li>Suppose \(\boldsymbol{y}_i = 0\) for some \(i\), then \(0 \le [\boldsymbol{Ay}]_i \le s \cdot 0 = 0 \Rightarrow [\boldsymbol{Ay}]_i = 0\). Let \(\boldsymbol{z} \triangleq \boldsymbol{Ay}\), then \(\boldsymbol{Az} \le s \boldsymbol{Ay} = s \boldsymbol{z}\). As \(\boldsymbol{z}_i = [\boldsymbol{Ay}]_i = 0, [\boldsymbol{Az}]_i = 0\). Repeat this and we get \([\boldsymbol{A}^k\boldsymbol{y}]_i = 0\) for any \(k\). But for \(k\) large enough \(\boldsymbol{A}^k &gt; \boldsymbol{0}\) which is a contradiction.</li> <li>Corollary 2.1.</li> <li>The “\(\Leftarrow\)” part is from Corollary 2.1 so we only need to prove the “\(\Rightarrow\)” part. Suppose \([\boldsymbol{Ay}]_i &lt; s \boldsymbol{y}_i\) for some \(i\). Define \(\boldsymbol{z} = s \boldsymbol{y} - \boldsymbol{Ay} \ge \boldsymbol{0}\) and \(\boldsymbol{z} \ne \boldsymbol{0}\). For \(k\) large enough, \(\boldsymbol{A}^k \boldsymbol{z} = s \boldsymbol{A}^k\boldsymbol{y} - \boldsymbol{A}^k\boldsymbol{Ay} = s \boldsymbol{x} - \boldsymbol{Ax} &gt; \boldsymbol{0}\) where \(\boldsymbol{x} = \boldsymbol{A}^k \boldsymbol{y} &gt; \boldsymbol{0}\). So \(\boldsymbol{Ax} &lt; s \boldsymbol{x} \Rightarrow \rho(\boldsymbol{A}) &lt; s = \rho(\boldsymbol{A})\) by Corollary 2.1, which is a contradiction.</li> </ol> <p><strong>Power control in wireless network:</strong> \(\boldsymbol{A} \ge \boldsymbol{0}\) and is irreducible. \(\boldsymbol{p}, \boldsymbol{b} \in \mathbb{R}^m\) and \(\boldsymbol{b} \ge \boldsymbol{0}\). Suppose \(\boldsymbol{A}, \boldsymbol{b}\) are known, then</p> \[\begin{cases} \boldsymbol{Ap} + \boldsymbol{b} \le \boldsymbol{p} \\ \boldsymbol{p} \ge \boldsymbol{0} \end{cases}\] <p>is feasible w.r.t \(\boldsymbol{p} \iff \rho(\boldsymbol{A}) &lt; 1\).</p> <p><strong>Proof:</strong> The “\(\Leftarrow\)” part. We show \((\boldsymbol{I} - \boldsymbol{A})^{-1}\) exists and \((\boldsymbol{I} - \boldsymbol{A})^{-1} &gt; \boldsymbol{0}\) element-wise, so \(\boldsymbol{p} = (\boldsymbol{I} - \boldsymbol{A})^{-1}\boldsymbol{b}\) is a feasible point.</p> \[(\boldsymbol{I} - \boldsymbol{A}) \lim_{k \to \infty} \sum_{i=0}^k \boldsymbol{A}^k = \lim_{k \to \infty} (\boldsymbol{I} - \boldsymbol{A}^{k+1}) = \boldsymbol{I}.\] <p>\(\lim_{k \to \infty} \boldsymbol{A}^{k+1} = \boldsymbol{0}\) by Lemma 3. So \((\boldsymbol{I} - \boldsymbol{A})^{-1} = \lim_{k \to \infty} \sum_{i=0}^k \boldsymbol{A}^k &gt; \boldsymbol{0}\) since \(\boldsymbol{A}\) is irreducible.</p> <p>The “\(\Rightarrow\)” part. As \(\boldsymbol{b} \ge \boldsymbol{0}\), we have \(\boldsymbol{Ap} \le \boldsymbol{p}, \boldsymbol{p} \ge \boldsymbol{0}\). By Subinvariance Theorem, \(\boldsymbol{p} &gt; \boldsymbol{0}\) and \(\rho(\boldsymbol{A}) \le 1\). If \(\rho(\boldsymbol{A}) = 1\), then \(\boldsymbol{Ap} = \boldsymbol{p}\) which is a contradiction as \(\boldsymbol{b} \ne \boldsymbol{0}\). So \(\rho(\boldsymbol{A}) &lt; 1\).</p>]]></content><author><name></name></author><category term="mathematics"/><category term="matrix"/><category term="eigenvalue"/><category term="spectral-radius"/><summary type="html"><![CDATA[a detailed proof of the PF theorem and an application]]></summary></entry><entry><title type="html">Deploying a Server for Bioinformatics Research</title><link href="https://torydeng.github.io/blog/2023/deploying-server/" rel="alternate" type="text/html" title="Deploying a Server for Bioinformatics Research"/><published>2023-07-11T00:00:00+00:00</published><updated>2023-07-11T00:00:00+00:00</updated><id>https://torydeng.github.io/blog/2023/deploying-server</id><content type="html" xml:base="https://torydeng.github.io/blog/2023/deploying-server/"><![CDATA[<p>Recently, our lab acquired a server equipped with a standard Ubuntu operating system from <code class="language-plaintext highlighter-rouge">Inspur</code>, and I am tasked with configuring it to fulfill the specific requirements of our bioinformatics research. Given that my expertise in Linux is limited, I dedicated several days to this endeavor, and eventually completed the deployment process. The purpose of this guide is to assist researchers facing similar demands in comprehending the steps to configure their servers. Additionally, it aims to address potential issues that may arise during the configuration process, along with their respective solutions.</p> <h2 id="create-a-new-user-with-root-privileges">Create a new user (with root privileges)</h2> <p>Typically, the server comes with a default user named after the vendor, in my instance, <code class="language-plaintext highlighter-rouge">inspur</code>. This user is a regular user but can gain root privileges by executing commands using the <code class="language-plaintext highlighter-rouge">sudo</code> prefix followed by typing their password. To create a custom account with similar privileges, follow these steps:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>useradd <span class="nt">-d</span> <span class="s2">"/home/&lt;user_name&gt;"</span> <span class="nt">-m</span> <span class="nt">-s</span> <span class="s2">"/bin/bash"</span> &lt;user_name&gt;
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">-d "/volume1/home/&lt;user_name&gt;"</code> will set <code class="language-plaintext highlighter-rouge">/volume1/home/&lt;user_name&gt;</code> as home directory of the new Ubuntu account.</li> <li><code class="language-plaintext highlighter-rouge">-m</code> will create the user’s home directory.</li> <li><code class="language-plaintext highlighter-rouge">-s "/bin/bash"</code>will set <code class="language-plaintext highlighter-rouge">/bin/bash</code> as login shell of the new account. This command will create a regular account <code class="language-plaintext highlighter-rouge">&lt;user_name&gt;</code>. If you want <code class="language-plaintext highlighter-rouge">&lt;user_name&gt;</code> to have root privileges, type:</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>useradd <span class="nt">-d</span> <span class="s2">"/home/&lt;user_name&gt;"</span> <span class="nt">-m</span> <span class="nt">-s</span> <span class="s2">"/bin/bash"</span> <span class="nt">-G</span> <span class="nb">sudo</span> &lt;user_name&gt;
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">-G sudo</code> ensures <code class="language-plaintext highlighter-rouge">&lt;user_name&gt;</code> to have admin access to the system.</li> </ul> <p>To set the password of the new account, conduct:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>passwd &lt;user_name&gt;
</code></pre></div></div> <p>After running the command, you will be prompted to type the password for the new account. Please note that Ubuntu will not display the password you are typing, either explicitly or implicitly (like dots). Just type the password you want to set and press <code class="language-plaintext highlighter-rouge">Enter</code>.</p> <h2 id="change-terminal-prompt-optional">Change terminal prompt (optional)</h2> <p>A beautiful terminal prompt can bring a beautiful day. To change the terminal prompt, execute</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~
vim .bashrc
</code></pre></div></div> <p>You will see a paragraph like this:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code># uncomment for a colored prompt, if the terminal has the capability; turned
# off by default to not distract the user: the focus in a terminal window
# should be on the output of commands, not on the prompt
#force_color_prompt=yes
</code></pre></div></div> <p>Uncomment the last line <code class="language-plaintext highlighter-rouge">#force_color_prompt=yes</code>. Below this paragraph you will also see some codes:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$color_prompt</span><span class="s2">"</span> <span class="o">=</span> <span class="nb">yes</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nv">PS1</span><span class="o">=</span><span class="s1">'${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ '</span>
<span class="k">else
    </span><span class="nv">PS1</span><span class="o">=</span><span class="s1">'${debian_chroot:+($debian_chroot)}\u@\h:\w\$ '</span>
<span class="k">fi</span>
</code></pre></div></div> <p>Modify the first <code class="language-plaintext highlighter-rouge">PS1</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">PS1</span><span class="o">=</span><span class="s1">'\[\033[35m\]\t\[\033[m\]-\[\033[36m\]\u\[\033[m\]@\[\033[32m\]\h:\[\033[33;1m\]\w\[\033[m\]\$ '</span>
</code></pre></div></div> <p>This is my <code class="language-plaintext highlighter-rouge">PS1</code> value. Save the <code class="language-plaintext highlighter-rouge">.bashrc</code> file, close your current terminal and open a new one. The terminal prompt will look like this:</p> <p><span style="color:magenta">23:02:02</span>-<span style="color:skyblue">tdeng</span>@<span style="color:green">inspur-NP5570M5:</span><span style="color:gold">~/data</span>$</p> <h2 id="enable-remote-access">Enable remote access</h2> <p>If you wish to access the server from outside its physical location, you need to enable remote access. In my case, I connected the server to the campus network, allowing me to access it from any location within the campus. To enable remote access, you need to install the <code class="language-plaintext highlighter-rouge">openssh-server</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>openssh-server
</code></pre></div></div> <p>If the firewall <code class="language-plaintext highlighter-rouge">UFW</code> is enabled, make sure to open the SSH port:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>ufw allow ssh
</code></pre></div></div> <p>To test whether you can access the server from a Windows system:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>telnet &lt;remote_ip&gt; &lt;remote_port&gt;
</code></pre></div></div> <p>You can find the IP address using:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip a
</code></pre></div></div> <p>When you log in to the server using the newly created user with bash, you might encounter an error like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/bin/xauth: file /home/&lt;user_name&gt;/.Xauthority does not exist
</code></pre></div></div> <p>Solution:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chown</span> &lt;user_name&gt;:&lt;user_name&gt; <span class="nt">-R</span> /home/&lt;user_name&gt;
</code></pre></div></div> <h2 id="connect-to-github">Connect to GitHub</h2> <p>I suppose you already have a GitHub account. Install <code class="language-plaintext highlighter-rouge">git</code> first:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>git
git <span class="nt">--version</span>
</code></pre></div></div> <p>Then configure certain information about your GitHub account:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git config <span class="nt">--global</span> user.name <span class="s2">"&lt;github_account_name&gt;"</span>
git config <span class="nt">--global</span> user.email <span class="s2">"&lt;github_account_email&gt;"</span>
</code></pre></div></div> <p>Connect to GitHub:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-keygen <span class="nt">-C</span> <span class="s2">"&lt;github_account_email&gt;"</span> <span class="nt">-t</span> rsa  <span class="c"># default: just press Enter 3 times</span>
<span class="nb">cd</span> ~/.ssh
vim id_rsa.pub  <span class="c"># open the id_rsa.pub file</span>
</code></pre></div></div> <p>Finally, copy the text in <code class="language-plaintext highlighter-rouge">id_rsa.pub</code>, log in GitHub, and create an SSH key at <code class="language-plaintext highlighter-rouge">Settings</code> → <code class="language-plaintext highlighter-rouge">SSH and GPG keys</code> → <code class="language-plaintext highlighter-rouge">New SSH key</code>.</p> <p>Test the connection:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-T</span> git@github.com
</code></pre></div></div> <h2 id="configure-the-python-environment">Configure the Python environment</h2> <h3 id="install-miniforge">Install miniforge</h3> <p>Instead of <code class="language-plaintext highlighter-rouge">Anaconda</code> I decide to use <code class="language-plaintext highlighter-rouge">Miniforge</code> to manage multiple <code class="language-plaintext highlighter-rouge">Python</code> environments. It has several advantages over Anaconda:</p> <ul> <li>The conda-forge channel is set as the default channel. So you don’t need to type <code class="language-plaintext highlighter-rouge">-c conda-forge</code>.</li> <li>It uses <a href="https://mamba.readthedocs.io/en/latest/index.html"><code class="language-plaintext highlighter-rouge">Mamba</code></a>, a very fast package manager (although <code class="language-plaintext highlighter-rouge">Anaconda</code> can also use <code class="language-plaintext highlighter-rouge">Mamba</code>, additional operations to set <code class="language-plaintext highlighter-rouge">conda-libmamba-solver</code> as the dfault solver are required).</li> </ul> <p>You can consider <code class="language-plaintext highlighter-rouge">Miniforge</code> as an alternative to <code class="language-plaintext highlighter-rouge">Anaconda</code>. You can replace the <code class="language-plaintext highlighter-rouge">conda</code> command with <code class="language-plaintext highlighter-rouge">mamba</code> for a better interface, or you can simply keep using the <code class="language-plaintext highlighter-rouge">conda</code> command for a seamless replacement. Below I will only show the former approach.</p> <p>To install <code class="language-plaintext highlighter-rouge">Miniforge</code>, just follow the installation guide in its <a href="https://github.com/conda-forge/miniforge">README</a>. Here I copy the core commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget <span class="s2">"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-</span><span class="si">$(</span><span class="nb">uname</span><span class="si">)</span><span class="s2">-</span><span class="si">$(</span><span class="nb">uname</span> <span class="nt">-m</span><span class="si">)</span><span class="s2">.sh"</span>
bash Miniforge3-<span class="si">$(</span><span class="nb">uname</span><span class="si">)</span>-<span class="si">$(</span><span class="nb">uname</span> <span class="nt">-m</span><span class="si">)</span>.sh
</code></pre></div></div> <p>I prefer install anaconda at <code class="language-plaintext highlighter-rouge">/usr/local/miniforge3</code> so that the environments can be shared by users (but only users with root can modify them). You don’t need to create this folder in advance. During the installation you will have chance to specify the installation directory.</p> <p>To initialize mamba, conduct</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/path/to/mamba init  <span class="c"># /usr/local/miniforge3/bin/mamba in my case</span>
</code></pre></div></div> <p>and reopen the terminal.</p> <h3 id="createdelete-environments">Create/delete environments</h3> <p>I recommend creating new environments and installing site packages with root privileges (<code class="language-plaintext highlighter-rouge">sudo su</code>) to restrict regular users from modifying the environments. If a regular user wants to update an environment, they should contact the system administrator for assistance. If he/she doesn’t and conduct a command secretly like</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba update <span class="nt">--all</span>
</code></pre></div></div> <p>he/she will proceed with the update plan but finally fail with error info:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Confirm changes: <span class="o">[</span>Y/n] y

frozendict                                          49.0kB @  60.0kB/s  0.8s
libzlib                                             61.6kB @  72.1kB/s  0.9s
lzo                                                171.4kB @ 168.4kB/s  1.0s
menuinst                                           137.7kB @ 131.9kB/s  1.0s
libsolv                                            470.7kB @ 324.7kB/s  1.4s
conda                                              961.2kB @ 558.1kB/s  0.9s

Downloading and Extracting Packages:

Preparing transaction: <span class="k">done
</span>Verifying transaction: failed
The current user does not have write permissions to the target environment.
  environment location: /usr/local/miniforge3
  uid: 1000
  gid: 1000



EnvironmentNotWritableError: The current user does not have write permissions to the target environment.
  environment location: /usr/local/miniforge3
  uid: 1000
  gid: 1000
</code></pre></div></div> <p>The commands for creating new environment are:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># create with a specified name</span>
mamba create <span class="nt">--name</span> &lt;new_env_name&gt; <span class="nv">python</span><span class="o">=</span>3.11 <span class="nt">--no-default-packages</span>
<span class="c"># create with a specified location; regular users can use this command to create an environment in their home directory</span>
mamba create <span class="nt">--prefix</span> /path/to/directory <span class="nv">python</span><span class="o">=</span>3.11 <span class="nt">--no-default-packages</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">--name &lt;new_env_name&gt;</code> will set the name of the new environment.</li> <li><code class="language-plaintext highlighter-rouge">--prefix /path/to/directory</code> will set the path to the directory where you want to create the environment</li> <li><code class="language-plaintext highlighter-rouge">python=3.11</code> means mamba will install <code class="language-plaintext highlighter-rouge">Python</code> 3.11 in the new environment.</li> <li><code class="language-plaintext highlighter-rouge">--no-default-packages</code> will only install <code class="language-plaintext highlighter-rouge">Python</code>. No other site packages will be included.</li> </ul> <p>I did not modify the <code class="language-plaintext highlighter-rouge">base</code> environment and proceeded to create two new environments: <code class="language-plaintext highlighter-rouge">jupyter</code> and <code class="language-plaintext highlighter-rouge">bio</code>. <code class="language-plaintext highlighter-rouge">jupyter</code> only contains packages related to jupyterhub, while <code class="language-plaintext highlighter-rouge">bio</code> encompasses all the necessary packages for research purposes.</p> <p>If you wish to delete an environment for any reason, utilize the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># delete with a specified name</span>
mamba remove <span class="nt">--name</span> &lt;env_name&gt; <span class="nt">--all</span>
<span class="c"># delete with a specified location</span>
mamba remove <span class="nt">--prefix</span> /path/to/directory <span class="nt">--all</span>
</code></pre></div></div> <h3 id="install-python-packages">Install Python packages</h3> <h4 id="jupyterhub">JupyterHub</h4> <p>You may want to install <code class="language-plaintext highlighter-rouge">JupyterHub</code>, which serves Jupyter notebook for multiple users.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba <span class="nb">install </span>jupyterhub jupyterlab notebook jupyter-lsp-python jupyterlab-lsp jupyterlab-git
</code></pre></div></div> <p>I recommend to install the <a href="https://github.com/jupyter-lsp/jupyterlab-lsp">jupyterlab-lsp</a>, a powerful coding assistance for JupyterLab. Another useful plugin is <a href="https://github.com/deshaw/jupyterlab-execute-time">jupyterlab-execute-time</a>, which can display cell timings in JupyterLab. Use the following command to install it:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba <span class="nb">install </span>jupyterlab_execute_time
</code></pre></div></div> <p>Refer to this <a href="https://jupyterhub.readthedocs.io/en/stable/tutorial/getting-started/config-basics.html">website</a> for the configuration of JupyterHub.</p> <p>Refer to this <a href="https://professorkazarinoff.github.io/jupyterhub-engr114/systemd/">website</a> for how to run JupyterHub as a system service.</p> <p>The key command to start the service on boot is:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl <span class="nb">enable </span>jupyterhub
</code></pre></div></div> <p>From version 5.0, you must modify the <code class="language-plaintext highlighter-rouge">jupyterhub_config.py</code> file to grants users who can successfully authenticate access to the Hub. Check <a href="https://jupyterhub.readthedocs.io/en/stable/tutorial/getting-started/authenticators-users-basics.html">this official tutorial</a> out.</p> <h4 id="adddelete-an-environment-as-a-kernel">Add/delete an environment as a kernel</h4> <p>To add an environment as a kernel:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba activate &lt;env_name&gt;  <span class="c"># or /path/to/directory if you create the env with --prefix</span>
mamba <span class="nb">install </span>ipykernel  <span class="c"># if the env doesn't contain this package</span>
python <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--name</span> &lt;kernel_name&gt;
</code></pre></div></div> <p>These commands add <code class="language-plaintext highlighter-rouge">&lt;env_name&gt;</code> environment as a kernel with name <code class="language-plaintext highlighter-rouge">&lt;kernel_name&gt;</code>. If your <code class="language-plaintext highlighter-rouge">Python</code> is 3.11, you may need to modify the last command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-Xfrozen_modules</span><span class="o">=</span>off <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--name</span> &lt;kernel_name&gt;
</code></pre></div></div> <p>To delete a kernel:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter kernelspec list
jupyter kernelspec uninstall &lt;kernel_name&gt;
</code></pre></div></div> <h4 id="other-packages">Other packages</h4> <p>Our research involves deep learning, so I need to install <code class="language-plaintext highlighter-rouge">pytorch</code> along with other required packages. <a href="https://rapids.ai/">RAPIDS</a> provides a series of packages that utilize GPUs. These packages are easier to install in a fresh environment so I recommend installing them first, following the <a href="https://docs.rapids.ai/install">Installation Guide</a>. <code class="language-plaintext highlighter-rouge">pytorch</code> can be installed simultaneously with the guide.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba <span class="nb">install </span>ipykernel ipywidgets <span class="c"># for Jupyter Notebook</span>
mamba <span class="nb">install </span>lightning  <span class="c"># for deep learning tasks</span>
mamba <span class="nb">install </span>pyro-ppl numpyro funsor arviz  <span class="c"># for probabilistic programming</span>
mamba <span class="nb">install </span>scanpy squidpy omicverse biopython rpy2 opencv   <span class="c"># for biological analysis</span>
mamba <span class="nb">install </span>anndata2ri <span class="nt">-c</span> bioconda  <span class="c"># for conversion between Python and R</span>
mamba <span class="nb">install </span>xgboost lightgbm catboost hdbscan optuna  <span class="c"># for machine learning tasks</span>
</code></pre></div></div> <p>Sometimes you may use <code class="language-plaintext highlighter-rouge">mamba search &lt;package_name&gt;</code> to search for a package with a specific build number. To install a specific version/build of a certain packages, conduct:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba <span class="nb">install</span> &lt;package_name&gt;<span class="o">=</span>&lt;version&gt;<span class="o">=</span>&lt;build_string&gt;
</code></pre></div></div> <h4 id="check-pytorchtensorflow">Check pytorch/tensorflow</h4> <p>If you are also a user of <code class="language-plaintext highlighter-rouge">pytorch</code> or <code class="language-plaintext highlighter-rouge">tensorflow</code> and you have one or more available GPU(s), you can execute the following codes to verify whether the GPU(s) can be recognized and utilized by the respective deep learning frameworks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># check pytorch and cuda in use
</span><span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">version</span><span class="p">.</span><span class="n">cuda</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device_count</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">current_device</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># check tensorflow
</span><span class="nf">print</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">list_physical_devices</span><span class="p">(</span><span class="sh">'</span><span class="s">GPU</span><span class="sh">'</span><span class="p">))</span>
</code></pre></div></div> <p>Here I also provide a script to ensure that <code class="language-plaintext highlighter-rouge">pytorch</code> can use the GPU(s) to train and test neural networks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torchvision</span>
<span class="kn">import</span> <span class="n">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>


<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="c1"># define image preprocessing
</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Pad</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomHorizontalFlip</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomCrop</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">()])</span>

<span class="c1"># download the CIFAR-10 dataset
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nc">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">data/</span><span class="sh">'</span><span class="p">,</span>
                                             <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                             <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                                             <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nc">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">data/</span><span class="sh">'</span><span class="p">,</span>
                                            <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                            <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">())</span>

<span class="c1"># load data
</span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                           <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>            <span class="c1"># number of subprocesses to use for data loading
</span>                                           <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>          <span class="c1"># the data loader will copy Tensors into CUDA pinned memory before returning them
</span>                                           <span class="n">prefetch_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>        <span class="c1"># number of batches loaded in advance by each worker
</span>                                           <span class="n">persistent_workers</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># the data loader will not shutdown the worker processes after a dataset has been consumed once
</span>                                           <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                          <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                          <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                          <span class="n">prefetch_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                          <span class="n">persistent_workers</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># 3x3 convolution kernel
</span><span class="k">def</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                     <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># define the residual block
</span><span class="k">class</span> <span class="nc">ResidualBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">downsample</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="n">downsample</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">downsample</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="n">residual</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># define the structure of ResNet
</span><span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="mi">16</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">avg_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">AvgPool2d</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_layer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">):</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="nf">conv3x3</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">))</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">block</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">block</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">avg_pool</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="n">model</span> <span class="o">=</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model = nn.DataParallel(model)  # uncomment this line if you have multiple GPUs
</span>

<span class="c1"># define loss function
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># function for the update of learning rate
</span><span class="k">def</span> <span class="nf">update_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

<span class="c1"># train the ResNet
</span><span class="n">total_step</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">curr_lr</span> <span class="o">=</span> <span class="n">learning_rate</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># forward step
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># backward step
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># report every 10 steps
</span>        <span class="nf">if </span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print </span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}</span><span class="sh">"</span>
                   <span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">total_step</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()))</span>

    <span class="c1"># update learning rate
</span>    <span class="nf">if </span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">curr_lr</span> <span class="o">/=</span> <span class="mi">3</span>
        <span class="nf">update_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">curr_lr</span><span class="p">)</span>

<span class="c1"># test the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy of the model on the test images: {} %</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">))</span>
</code></pre></div></div> <p>Note that if you have multiple GPUs, you need to uncomment the line below the code which creates the model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># model = nn.DataParallel(model)
</span></code></pre></div></div> <p>You can use this command to monitor the GPU(s) during training:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch <span class="nt">-n</span> 0.2 nvidia-smi
</code></pre></div></div> <h2 id="configure-the-r-environment">Configure the R environment</h2> <h3 id="install-r">Install R</h3> <p>The simplest way to install <code class="language-plaintext highlighter-rouge">R</code> &gt;= 4.0 is to run</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>r-base
</code></pre></div></div> <p>However, it will not bring you the latest version of <code class="language-plaintext highlighter-rouge">R</code>. To get the latest version of <code class="language-plaintext highlighter-rouge">R</code>, refer to <a href="https://phoenixnap.com/kb/install-r-ubuntu">this website</a> and <a href="https://cran.r-project.org/bin/linux/ubuntu/fullREADME.html">this offical website</a>. Here I copy the core commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># update the package list from repositories</span>
<span class="nb">sudo </span>apt update
<span class="c"># install without confirmation</span>
<span class="nb">sudo </span>apt <span class="nb">install </span>software-properties-common dirmngr <span class="nt">-y</span>
<span class="c"># download the R project public key and add it to the trusted list of GPG keys used by apt</span>
wget <span class="nt">-qO-</span> https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc
<span class="c"># verify the key; the fingerprint should be E298A3A825C0D65DFD57CBB651716619E084DAB9</span>
gpg <span class="nt">--show-keys</span> /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc
<span class="c"># add the CRAN repository for your version of Ubuntu to the list of sources apt uses to install packages</span>
<span class="nb">sudo </span>add-apt-repository <span class="s2">"deb https://cloud.r-project.org/bin/linux/ubuntu </span><span class="si">$(</span>lsb_release <span class="nt">-cs</span><span class="si">)</span><span class="s2">-cran40/"</span>
<span class="c"># install R and its development packages</span>
<span class="nb">sudo </span>apt <span class="nb">install </span>r-base r-base-dev <span class="nt">-y</span>
</code></pre></div></div> <h3 id="install-rstudio">Install RStudio</h3> <p>Follow the <a href="https://posit.co/download/rstudio-server/">official installation guide</a>. This should be easier than installing <code class="language-plaintext highlighter-rouge">JupyterHub</code>.</p> <h3 id="install-r-packages">Install R packages</h3> <p>As an example, let’s install one of the most prevalent R package in the field of single-cell genomics, <a href="https://satijalab.org/seurat/index.html"><code class="language-plaintext highlighter-rouge">Seurat</code></a> (version 5). Before the installation, you need to install some system-level dependencies first:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>build-essential libssl-dev libcurl4-openssl-dev libxml2-dev libfontconfig1-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev libhdf5-dev libgsl-dev
</code></pre></div></div> <p>Then the process of installing <code class="language-plaintext highlighter-rouge">Seurat</code> should be very smooth:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>R
</code></pre></div></div> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chooseCRANmirror</span><span class="p">(</span><span class="n">graphics</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"Seurat"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p>Additional packages can be installed to enhance the functionality of <code class="language-plaintext highlighter-rouge">Seurat</code>. Check the <a href="https://satijalab.org/seurat/articles/install_v5">official intallation tutorial</a> of <code class="language-plaintext highlighter-rouge">Seurat</code> out. If you intend to install an extremely large R package, you’d better set a longer timeout:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">options</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="m">999</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"&lt;large_package&gt;"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p>Other useful R packages are:</p> <ul> <li><code class="language-plaintext highlighter-rouge">devtools</code> for package development</li> <li><code class="language-plaintext highlighter-rouge">tidyverse</code> for geneal data analysis</li> <li><code class="language-plaintext highlighter-rouge">tidyomics</code> for omics data analysis</li> <li><code class="language-plaintext highlighter-rouge">ComplexHeatmap</code> for visualizing matrices</li> </ul> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">install.packages</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="s2">"devtools"</span><span class="p">,</span><span class="w"> </span><span class="s2">"tidyverse"</span><span class="p">))</span><span class="w">
</span><span class="n">BiocManager</span><span class="o">::</span><span class="n">install</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="s2">"tidyomics"</span><span class="p">,</span><span class="w"> </span><span class="s2">"ComplexHeatmap"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div> <p>When running <code class="language-plaintext highlighter-rouge">devtools::install_github()</code>, you may encounter an error complaining that the API rate limit has been exceeded. The solution to this issue is to create a GitHub token.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">usethis</span><span class="o">::</span><span class="n">create_github_token</span><span class="p">()</span><span class="w">
</span></code></pre></div></div> <p>Run this code in your RStudio console and log in to your GitHub account. Click <code class="language-plaintext highlighter-rouge">Settings</code> → <code class="language-plaintext highlighter-rouge">Developer settings</code> → <code class="language-plaintext highlighter-rouge">Personal access token</code> → <code class="language-plaintext highlighter-rouge">Tokens (classic)</code> (if the browser does not automatically direct you to this page) and generate a token. Run</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gitcreds</span><span class="o">::</span><span class="n">gitcreds_set</span><span class="p">()</span><span class="w">
</span></code></pre></div></div> <p>also in your RStudio console (or in the terminal if you are <code class="language-plaintext highlighter-rouge">sudo</code>) to add the token. The limit should be relaxed and you can continue the installation, and you can see a message like:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using GitHub PAT from the git credential store.
Downloading GitHub repo &lt;github_username&gt;/&lt;repo_name&gt;@&lt;branch_name&gt;
</code></pre></div></div> <h2 id="synchronize-data">Synchronize data</h2> <p>Refer to <a href="https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories">this website</a> for detailed instructions on how to synchronize data stored on another server.</p> <p>The key command is</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rsync <span class="nt">-r</span> /path/to/sync/ &lt;username&gt;@&lt;remote_host&gt;:&lt;destination_directory&gt;
</code></pre></div></div> <p>which “pushes” all contents in <code class="language-plaintext highlighter-rouge">/path/to/sync/</code> from the system you are logging in to <code class="language-plaintext highlighter-rouge">&lt;destination_directory&gt;</code> in the target system.</p> <p>If you are synchronizing a large file, you may want to monitor the process:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch <span class="nt">-n</span> &lt;time_interval&gt; <span class="nb">du</span> <span class="nt">-sh</span> /path/to/large/file
</code></pre></div></div> <h2 id="install-some-basic-fonts">Install some basic fonts</h2> <p>By default, some basic fonts in Windows are not installed in Linux, such as <code class="language-plaintext highlighter-rouge">Arial</code> and <code class="language-plaintext highlighter-rouge">Times New Roman</code>. These fonts are commonly used in papers and websites, and having them installed can improve the display of figures that expect these fonts to be available. You can install them by:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>msttcorefonts
<span class="nb">rm</span> <span class="nt">-rf</span> ~/.cache/matplotlib
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">msttcorefonts</code> package is a collection of TrueType fonts from Microsoft. The second command clears the matplotlib cache directory located in the hidden <code class="language-plaintext highlighter-rouge">.cache</code> directory in the user’s home directory.</p> <h2 id="troubleshooting">Troubleshooting</h2> <h3 id="driverlibrary-version-mismatch">Driver/library version mismatch</h3> <p>When you run <code class="language-plaintext highlighter-rouge">nvidia-smi</code>, you may get</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Failed to initialize NVML: Driver/library version mismatch
</code></pre></div></div> <p><a href="https://stackoverflow.com/questions/43022843/nvidia-nvml-driver-library-version-mismatch/45319156#45319156">This answer</a> from stackoverflow may help. Briefly you can either reboot or unload the <code class="language-plaintext highlighter-rouge">nvidia</code> module. However, if both the ways can’t help, you need to reinstall the nvidia drivers:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt purge nvidia<span class="k">*</span> libnvidia<span class="k">*</span>
<span class="nb">sudo </span>ubuntu-drivers autoinstall
</code></pre></div></div> <p>and then <code class="language-plaintext highlighter-rouge">sudo reboot</code> your server.</p> <h3 id="upgrade-nvidia-drivers">Upgrade Nvidia drivers</h3> <p>You can upgrade the Nvidia driver by these steps:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># clean the installed version</span>
<span class="nb">sudo </span>apt purge <span class="k">*</span>nvidia<span class="k">*</span> <span class="nt">-y</span>
<span class="nb">sudo </span>apt remove <span class="k">*</span>nvidia<span class="k">*</span> <span class="nt">-y</span>
<span class="nb">sudo rm</span> /etc/apt/sources.list.d/cuda<span class="k">*</span>
<span class="nb">sudo </span>apt autoremove <span class="nt">-y</span> <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt autoclean <span class="nt">-y</span>
<span class="nb">sudo rm</span> <span class="nt">-rf</span> /usr/local/cuda<span class="k">*</span>

<span class="c"># find recommended driver versions</span>
ubuntu-drivers devices  <span class="c"># or sudo apt search nvidia</span>

<span class="c"># install the lastest version (replace `550` with the latest version number)</span>
<span class="nb">sudo </span>apt <span class="nb">install </span>libnvidia-common-550-server libnvidia-gl-550-server nvidia-driver-550-server <span class="nt">-y</span>

<span class="c"># reboot</span>
<span class="nb">sudo </span>reboot now
</code></pre></div></div> <p>After reboot, you can check whether the new driver works by <code class="language-plaintext highlighter-rouge">nvidia-smi</code> ( although you may be required to also install <code class="language-plaintext highlighter-rouge">nvidia-utils-550-server</code>). Theoretically the command <code class="language-plaintext highlighter-rouge">nvidia-smi</code> should work, but you may still get an error message</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.
</code></pre></div></div> <p>even you have installed the latest driver. In this case you can try reinstalling kernel headers:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">--reinstall</span> linux-headers-<span class="si">$(</span><span class="nb">uname</span> <span class="nt">-r</span><span class="si">)</span>
</code></pre></div></div> <p>If you encounter some errors like <code class="language-plaintext highlighter-rouge">cc: error: unrecognized command-line option ‘-ftrivial-auto-var-init=zero’</code>, you can use <code class="language-plaintext highlighter-rouge">gcc 12</code> instead of <code class="language-plaintext highlighter-rouge">gcc 11</code> by</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>gcc-12
<span class="nb">sudo </span>update-alternatives <span class="nt">--install</span> /usr/bin/gcc gcc /usr/bin/gcc-12 12
</code></pre></div></div> <p>After the headers are reinstalled, you need to <code class="language-plaintext highlighter-rouge">sudo reboot</code> the server. Then <code class="language-plaintext highlighter-rouge">nvidia-smi</code> should work now.</p> <p>Now, your server should be well-suited for your bioinformatics research and you know what to do when things go wrong. Enjoy it!</p>]]></content><author><name></name></author><category term="computer"/><category term="deployment"/><category term="server"/><category term="Ubuntu"/><summary type="html"><![CDATA[how to deploy a server for bioinformatics research]]></summary></entry></feed>