<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Deploying a Server for Bioinformatics Research | Tao Deng </title> <meta name="author" content="Tao Deng"> <meta name="description" content="how to deploy a server for bioinformatics research"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://torydeng.github.io/blog/2023/deploying-server/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tao </span> Deng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/English_Resume.pdf">CV (English) </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/Chinese_Resume.pdf">CV (Chinese) </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Deploying a Server for Bioinformatics Research</h1> <p class="post-meta"> October 14, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/deployment"> <i class="fa-solid fa-hashtag fa-sm"></i> deployment</a>   <a href="/blog/tag/server"> <i class="fa-solid fa-hashtag fa-sm"></i> server</a>   <a href="/blog/tag/ubuntu"> <i class="fa-solid fa-hashtag fa-sm"></i> Ubuntu</a>     ·   <a href="/blog/category/computer"> <i class="fa-solid fa-tag fa-sm"></i> computer</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Recently, our lab acquired a server equipped with a standard Ubuntu operating system from <code class="language-plaintext highlighter-rouge">Inspur</code>, and I am tasked with configuring it to fulfill the specific requirements of our bioinformatics research. Given that my expertise in Linux is limited, I dedicated several days to this endeavor, and eventually completed the deployment process. The purpose of this guide is to assist researchers facing similar demands in comprehending the steps to configure their servers. Additionally, it aims to address potential issues that may arise during the configuration process, along with their respective solutions.</p> <h2 id="create-a-new-user-with-root-privileges">Create a new user (with root privileges)</h2> <p>Typically, the server comes with a default user named after the vendor, in my instance, <code class="language-plaintext highlighter-rouge">inspur</code>. This user is a regular user but can gain root privileges by executing commands using the <code class="language-plaintext highlighter-rouge">sudo</code> prefix followed by typing its password. To create a custom account with similar privileges, follow these steps:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>useradd <span class="nt">-d</span> <span class="s2">"/home/&lt;user_name&gt;"</span> <span class="nt">-m</span> <span class="nt">-s</span> <span class="s2">"/bin/bash"</span> &lt;user_name&gt;
</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">-d "/volume1/home/&lt;user_name&gt;"</code> will set <code class="language-plaintext highlighter-rouge">/volume1/home/&lt;user_name&gt;</code> as home directory of the new Ubuntu account.</li> <li> <code class="language-plaintext highlighter-rouge">-m</code> will create the user’s home directory.</li> <li> <code class="language-plaintext highlighter-rouge">-s "/bin/bash"</code>will set <code class="language-plaintext highlighter-rouge">/bin/bash</code> as login shell of the new account. This command will create a regular account <code class="language-plaintext highlighter-rouge">&lt;user_name&gt;</code>. If you want <code class="language-plaintext highlighter-rouge">&lt;user_name&gt;</code> to have root privileges, type:</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>useradd <span class="nt">-d</span> <span class="s2">"/home/&lt;user_name&gt;"</span> <span class="nt">-m</span> <span class="nt">-s</span> <span class="s2">"/bin/bash"</span> <span class="nt">-G</span> <span class="nb">sudo</span> &lt;user_name&gt;
</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">-G sudo</code> ensures <code class="language-plaintext highlighter-rouge">&lt;user_name&gt;</code> to have admin access to the system.</li> </ul> <p>To set the password of the new account, conduct:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>passwd &lt;user_name&gt;
</code></pre></div></div> <p>After running the command, you will be prompted to type your password. Please note that Ubuntu will not display the password you are typing explicitly or implicitly (like dots). Just type the password you want to set and press <code class="language-plaintext highlighter-rouge">Enter</code>.</p> <h2 id="change-terminal-prompt-optional">Change terminal prompt (optional)</h2> <p>A beautiful terminal prompt can bring a beautiful day. To change the terminal prompt, conduct</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~
vim .bashrc
</code></pre></div></div> <p>You will see a paragraph like this:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code># uncomment for a colored prompt, if the terminal has the capability; turned
# off by default to not distract the user: the focus in a terminal window
# should be on the output of commands, not on the prompt
#force_color_prompt=yes
</code></pre></div></div> <p>Uncomment the last line <code class="language-plaintext highlighter-rouge">#force_color_prompt=yes</code>. Below this paragraph you will also see some codes:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$color_prompt</span><span class="s2">"</span> <span class="o">=</span> <span class="nb">yes</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nv">PS1</span><span class="o">=</span><span class="s1">'${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ '</span>
<span class="k">else
    </span><span class="nv">PS1</span><span class="o">=</span><span class="s1">'${debian_chroot:+($debian_chroot)}\u@\h:\w\$ '</span>
<span class="k">fi</span>
</code></pre></div></div> <p>Modify the first <code class="language-plaintext highlighter-rouge">PS1</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">PS1</span><span class="o">=</span><span class="s1">'\[\033[35m\]\t\[\033[m\]-\[\033[36m\]\u\[\033[m\]@\[\033[32m\]\h:\[\033[33;1m\]\w\[\033[m\]\$ '</span>
</code></pre></div></div> <p>This is my <code class="language-plaintext highlighter-rouge">PS1</code> value. Save the <code class="language-plaintext highlighter-rouge">.bashrc</code> file, close your current terminal and open a new one. The terminal prompt will look like this:</p> <p><span style="color:magenta">23:02:02</span>-<span style="color:skyblue">tdeng</span>@<span style="color:green">inspur-NP5570M5:</span><span style="color:gold">~/data</span>$ .</p> <h2 id="enable-remote-access">Enable remote access</h2> <p>If you wish to access the server from outside its physical location, you need to enable remote access. In my case, I connected the server to the campus network, allowing me to access it from any location within the campus. To enable remote access, you need to install the <code class="language-plaintext highlighter-rouge">openssh-server</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>openssh-server
</code></pre></div></div> <p>If the firewall <code class="language-plaintext highlighter-rouge">UFW</code> is enabled, make sure to open the SSH port:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>ufw allow ssh
</code></pre></div></div> <p>To test whether you can access the server from a Windows system:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>telnet &lt;remote_ip&gt; &lt;remote_port&gt;
</code></pre></div></div> <p><a href="https://linuxize.com/post/how-to-enable-ssh-on-ubuntu-20-04/" rel="external nofollow noopener" target="_blank">This website</a> might be useful.</p> <p>When you log in to the server using the newly created user with bash, you might encounter an error like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/bin/xauth: file /home/&lt;user_name&gt;/.Xauthority does not exist
</code></pre></div></div> <p>Solution:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chown</span> &lt;user_name&gt;:&lt;user_name&gt; <span class="nt">-R</span> /home/&lt;user_name&gt;
</code></pre></div></div> <h2 id="connect-to-github">Connect to GitHub</h2> <p>I suppose you already have a GitHub account. Install <code class="language-plaintext highlighter-rouge">git</code> first:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>git
git <span class="nt">--version</span>
</code></pre></div></div> <p>Then configure certain information about your GitHub account:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git config <span class="nt">--global</span> user.name <span class="s2">"&lt;github_account_name&gt;"</span>
git config <span class="nt">--global</span> user.email <span class="s2">"&lt;github_account_email&gt;"</span>
</code></pre></div></div> <p>Connect to GitHub:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-keygen <span class="nt">-C</span> <span class="s2">"&lt;github_account_email&gt;"</span> <span class="nt">-t</span> rsa  <span class="c"># default: just press Enter 3 times</span>
<span class="nb">cd</span> ~/.ssh
vim id_rsa.pub  <span class="c"># open the id_rsa.pub file</span>
</code></pre></div></div> <p>Finally, copy the text in <code class="language-plaintext highlighter-rouge">id_rsa.pub</code>, log in GitHub, and create an SSH key at <code class="language-plaintext highlighter-rouge">Settings</code> → <code class="language-plaintext highlighter-rouge">SSH and GPG keys</code> → <code class="language-plaintext highlighter-rouge">New SSH key</code>.</p> <p>Test the connection:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-T</span> git@github.com
</code></pre></div></div> <h2 id="configure-the-python-environment">Configure the python environment</h2> <h3 id="install-anaconda">Install Anaconda</h3> <p>Just follow the <a href="https://docs.anaconda.com/free/anaconda/install/linux/" rel="external nofollow noopener" target="_blank">official installation guide</a>. I prefer install anaconda at <code class="language-plaintext highlighter-rouge">/usr/local/anaconda3</code>. You don’t need to create this folder in advance. During the installation you will have chance to specify the installation directory.</p> <p>To initialize conda, conduct</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/path/to/conda init  <span class="c"># /usr/local/anaconda3/bin/conda in my case</span>
</code></pre></div></div> <p>and reopen the terminal.</p> <h3 id="createdelete-environments">Create/delete environments</h3> <p>I recommend creating new environments and installing site packages with root privileges (<code class="language-plaintext highlighter-rouge">sudo su</code>) to restrict regular users from modifying the environments. If a regular user wants to update an environment, they should contact the system administrator for assistance. If he/she doesn’t and conduct a command secretly like</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda update <span class="nt">--all</span>
</code></pre></div></div> <p>he/she will proceed with the update plan but finally fail with error info:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Proceed <span class="o">([</span>y]/n<span class="o">)</span>? y


Downloading and Extracting Packages

Preparing transaction: <span class="k">done
</span>Verifying transaction: failed

EnvironmentNotWritableError: The current user does not have write permissions to the target environment.
  environment location: /usr/local/anaconda3/envs/bio
  uid: 1001
  gid: 1001
</code></pre></div></div> <p>The command for creating new environment is:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">--name</span> &lt;new_env_name&gt; <span class="nv">python</span><span class="o">=</span>3.11 <span class="nt">--no-default-packages</span>
</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">--name &lt;new_env_name&gt;</code> will set the name of the new environment.</li> <li> <code class="language-plaintext highlighter-rouge">python=3.11</code> means conda will install python 3.11 in the new environment.</li> <li> <code class="language-plaintext highlighter-rouge">--no-default-packages</code> will only install python. No other site packages will be included.</li> </ul> <p>I did not modify the <code class="language-plaintext highlighter-rouge">base</code> environment and proceeded to create two new environments: <code class="language-plaintext highlighter-rouge">jupyter</code> and <code class="language-plaintext highlighter-rouge">bio</code>. <code class="language-plaintext highlighter-rouge">jupyter</code> only contains packages related to jupyterhub, while <code class="language-plaintext highlighter-rouge">bio</code> encompasses all the necessary packages for research purposes.</p> <p>If you wish to delete an environment for any reason, utilize the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda remove <span class="nt">--name</span> &lt;env_name&gt; <span class="nt">--all</span>
</code></pre></div></div> <h3 id="install-python-packages">Install python packages</h3> <h4 id="mamba">Mamba</h4> <p>Before install other python packages, I recommend to install <code class="language-plaintext highlighter-rouge">conda-libmamba-solver</code> first, which is a faster solver:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda update <span class="nt">-n</span> &lt;env_name&gt; conda
conda <span class="nb">install</span> <span class="nt">-n</span> &lt;env_name&gt; conda-libmamba-solver
conda config <span class="nt">--set</span> solver libmamba
</code></pre></div></div> <p>The plugin needs to be present in the same environment you use <code class="language-plaintext highlighter-rouge">conda</code> from; most of the time, this is your <code class="language-plaintext highlighter-rouge">base</code> environment.</p> <h4 id="jupyterhub">JupyterHub</h4> <p>You may want to use <code class="language-plaintext highlighter-rouge">JupyterHub</code>.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge jupyterhub jupyterlab notebook jupyter-lsp-python jupyterlab-lsp
</code></pre></div></div> <p>I recommend to install the <a href="https://github.com/jupyter-lsp/jupyterlab-lsp" rel="external nofollow noopener" target="_blank">jupyterlab-lsp</a>, a powerful coding assistance for JupyterLab. Another useful plugin is <a href="https://github.com/deshaw/jupyterlab-execute-time" rel="external nofollow noopener" target="_blank">jupyterlab-execute-time</a>, which can display cell timings in JupyterLab. Use the following command to install it:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge jupyterlab_execute_time
</code></pre></div></div> <p>Refer to <a href="https://jupyterhub.readthedocs.io/en/stable/tutorial/getting-started/config-basics.html" rel="external nofollow noopener" target="_blank">this website</a> for the configuration of JupyterHub.</p> <p>Refer to <a href="https://professorkazarinoff.github.io/jupyterhub-engr114/systemd/" rel="external nofollow noopener" target="_blank">this website</a> for how to run JupyterHub as a system service.</p> <p>Refer to <a href="https://linuxconfig.org/how-to-start-service-on-boot-on-ubuntu-20-04" rel="external nofollow noopener" target="_blank">this website</a> for how to start the service on boot.</p> <h4 id="adddelete-an-environment-as-a-kernel">Add/delete an environment as a kernel</h4> <p>To add an environment as a kernel:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda activate &lt;env_name&gt;
conda <span class="nb">install </span>ipykernel  <span class="c"># if the env doesn't contain this package</span>
python <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--name</span> &lt;kernel_name&gt;
</code></pre></div></div> <p>These commands add <code class="language-plaintext highlighter-rouge">&lt;env_name&gt;</code> environment as a kernel with name <code class="language-plaintext highlighter-rouge">&lt;kernel_name&gt;</code>. If your Python is 3.11, you may need to modify the last command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-Xfrozen_modules</span><span class="o">=</span>off <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--name</span> bio
</code></pre></div></div> <p>To delete a kernel:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter kernelspec list
jupyter kernelspec uninstall &lt;kernel_name&gt;
</code></pre></div></div> <h4 id="other-packages">Other packages</h4> <p>Our research involves deep learning, so I need to install <code class="language-plaintext highlighter-rouge">pytorch</code> and <code class="language-plaintext highlighter-rouge">tensorflow</code> along with other required packages:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-c</span> pytorch <span class="nt">-c</span> nvidia <span class="nt">-c</span> conda-forge scvi-tools tensorflow torchvision torchaudio
conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge scanpy squidpy ipykernel ipywidgets rpy2 opencv biopython
conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge xgboost lightgbm catboost
</code></pre></div></div> <p>Note: <code class="language-plaintext highlighter-rouge">pytorch</code> and <code class="language-plaintext highlighter-rouge">pytorch-lightning</code> are dependencies of <code class="language-plaintext highlighter-rouge">scvi-tools</code> so you don’t need to install these two packages again.</p> <p>Sometimes you may use <code class="language-plaintext highlighter-rouge">conda search &lt;package_name&gt;</code> to search for a package with a specific build number. To install a specific version/build of a certain packages, conduct:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> &lt;package_name&gt;<span class="o">=</span>&lt;version&gt;<span class="o">=</span>&lt;build_string&gt;
</code></pre></div></div> <h4 id="check-pytorchtensorflow">Check pytorch/tensorflow</h4> <p>If you are also a user of <code class="language-plaintext highlighter-rouge">pytorch</code> or <code class="language-plaintext highlighter-rouge">tensorflow</code> and you have one or more available GPU(s), you can execute the following codes to verify whether the GPU(s) can be recognized and utilized by the respective deep learning frameworks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># check pytorch
</span><span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device_count</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">current_device</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># check tensorflow
</span><span class="nf">print</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">list_physical_devices</span><span class="p">(</span><span class="sh">'</span><span class="s">GPU</span><span class="sh">'</span><span class="p">))</span>
</code></pre></div></div> <p>Here I also provide a script to ensure that <code class="language-plaintext highlighter-rouge">pytorch</code> can use the GPU(s) to train and test neural networks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torchvision</span>
<span class="kn">import</span> <span class="n">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>


<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="c1"># define image preprocessing
</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Pad</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomHorizontalFlip</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomCrop</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">()])</span>

<span class="c1"># download the CIFAR-10 dataset
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nc">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">data/</span><span class="sh">'</span><span class="p">,</span>
                                             <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                             <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                                             <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nc">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">data/</span><span class="sh">'</span><span class="p">,</span>
                                            <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                            <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">())</span>

<span class="c1"># load data
</span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                           <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>            <span class="c1"># number of subprocesses to use for data loading
</span>                                           <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>          <span class="c1"># the data loader will copy Tensors into CUDA pinned memory before returning them
</span>                                           <span class="n">prefetch_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>        <span class="c1"># number of batches loaded in advance by each worker
</span>                                           <span class="n">persistent_workers</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># the data loader will not shutdown the worker processes after a dataset has been consumed once
</span>                                           <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                          <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                          <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                          <span class="n">prefetch_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                          <span class="n">persistent_workers</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># 3x3 convolution kernel
</span><span class="k">def</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                     <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># define the residual block
</span><span class="k">class</span> <span class="nc">ResidualBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">downsample</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="n">downsample</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">downsample</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="n">residual</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># define the structure of ResNet
</span><span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="mi">16</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">avg_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">AvgPool2d</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_layer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">):</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="nf">conv3x3</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">))</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">block</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">block</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">avg_pool</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="n">model</span> <span class="o">=</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model = nn.DataParallel(model)  # uncomment this line if you have multiple GPUs
</span>

<span class="c1"># define loss function
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># function for the update of learning rate
</span><span class="k">def</span> <span class="nf">update_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

<span class="c1"># train the ResNet
</span><span class="n">total_step</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">curr_lr</span> <span class="o">=</span> <span class="n">learning_rate</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># forward step
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># backward step
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># report every 10 steps
</span>        <span class="nf">if </span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print </span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}</span><span class="sh">"</span>
                   <span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">total_step</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()))</span>

    <span class="c1"># update learning rate
</span>    <span class="nf">if </span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">curr_lr</span> <span class="o">/=</span> <span class="mi">3</span>
        <span class="nf">update_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">curr_lr</span><span class="p">)</span>

<span class="c1"># test the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy of the model on the test images: {} %</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">))</span>
</code></pre></div></div> <p>Note that if you have multiple GPUs, you need to uncomment the line below the code which creates the model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># model = nn.DataParallel(model)
</span></code></pre></div></div> <p>You can use this command to monitor the GPU(s) during training:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch <span class="nt">-n</span> 0.2 nvidia-smi
</code></pre></div></div> <h2 id="configure-the-r-environment">Configure the R environment</h2> <h3 id="install-r">Install R</h3> <p>Refer to <a href="https://www.digitalocean.com/community/tutorials/how-to-install-r-on-ubuntu-22-04" rel="external nofollow noopener" target="_blank">this website</a> for instructions on adding the external repository maintained by CRAN for <code class="language-plaintext highlighter-rouge">APT</code> and subsequently installing <code class="language-plaintext highlighter-rouge">R</code>.</p> <h3 id="install-rstudio">Install RStudio</h3> <p>Follow the <a href="https://posit.co/download/rstudio-server/" rel="external nofollow noopener" target="_blank">official installation guide</a>. This should be easier than installing <code class="language-plaintext highlighter-rouge">JupyterHub</code>.</p> <h3 id="install-r-packages">Install R packages</h3> <p>As an example, let’s install one of the most famous R package in the field of single-cell genomics, <a href="https://satijalab.org/seurat/index.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">Seurat</code></a>. Before the installation, you need to install some system-level dependencies first:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>cmake pandoc pandoc-citeproc libcurl4-openssl-dev libfontconfig1-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev imagemagick libmagick++-dev libhdf5-dev libgsl-dev libssl-dev
</code></pre></div></div> <p>Then the process of installing <code class="language-plaintext highlighter-rouge">Seurat</code> should be very smooth:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chooseCRANmirror</span><span class="p">(</span><span class="n">graphics</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"devtools"</span><span class="p">)</span><span class="w">
</span><span class="n">remotes</span><span class="o">::</span><span class="n">install_github</span><span class="p">(</span><span class="s2">"satijalab/seurat"</span><span class="p">,</span><span class="w"> </span><span class="s2">"seurat5"</span><span class="p">,</span><span class="w"> </span><span class="n">quiet</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p>If you intend to install an extremely large R package, you’d better set a longer timeout:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">options</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="m">999</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"&lt;large_package&gt;"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p>When running <code class="language-plaintext highlighter-rouge">devtools::install_github()</code>, you may encounter an error complaining that the API rate limit has been exceeded. The solution to this issue is to create a GitHub token.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">usethis</span><span class="o">::</span><span class="n">create_github_token</span><span class="p">()</span><span class="w">
</span></code></pre></div></div> <p>Run this code and log in to your GitHub account. Click <code class="language-plaintext highlighter-rouge">Settings</code> → <code class="language-plaintext highlighter-rouge">Developer settings</code> → <code class="language-plaintext highlighter-rouge">Personal access token</code> → <code class="language-plaintext highlighter-rouge">Tokens (classic)</code> and generate a token. Run</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">usethis</span><span class="o">::</span><span class="n">edit_r_environ</span><span class="p">()</span><span class="w">
</span></code></pre></div></div> <p>to open the <code class="language-plaintext highlighter-rouge">.Renviron</code> file, in which type</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GITHUB_PAT=&lt;your_personal_key&gt;
</code></pre></div></div> <p>and save the file. Quit the current R session and start a new session. The error should be solved.</p> <h2 id="synchronize-data">Synchronize data</h2> <p>Refer to <a href="https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories" rel="external nofollow noopener" target="_blank">this website</a> for detailed instructions on how to synchronize data stored on another server.</p> <p>The key command is</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rsync <span class="nt">-r</span> /path/to/sync/ &lt;username&gt;@&lt;remote_host&gt;:&lt;destination_directory&gt;
</code></pre></div></div> <p>which “pushes” a directory from the system you are logging in to another system.</p> <p>If you are synchronizing a large file, you may want to monitor the process:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch <span class="nt">-n</span> &lt;time_interval&gt; <span class="nb">du</span> <span class="nt">-sh</span> /path/to/large/file
</code></pre></div></div> <p>Now, your server should be well-suited for your bioinformatics research. Enjoy it!</p> </div> </article> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Tao Deng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 19, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>