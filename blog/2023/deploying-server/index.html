<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Deploying a Server for Bioinformatics Research | Tao Deng </title> <meta name="author" content="Tao Deng"> <meta name="description" content="how to deploy a server for bioinformatics research"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://torydeng.github.io/blog/2023/deploying-server/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Tao</span> Deng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/English_Resume.pdf">CV (English) </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/Chinese_Resume.pdf">CV (Chinese) </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Deploying a Server for Bioinformatics Research</h1> <p class="post-meta"> Created on July 11, 2023 , last updated on July 09, 2025 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/deployment"> <i class="fa-solid fa-hashtag fa-sm"></i> deployment</a>   <a href="/blog/tag/server"> <i class="fa-solid fa-hashtag fa-sm"></i> server</a>   <a href="/blog/tag/ubuntu"> <i class="fa-solid fa-hashtag fa-sm"></i> Ubuntu</a>   ·   <a href="/blog/category/computer"> <i class="fa-solid fa-tag fa-sm"></i> computer</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Recently, our lab acquired a server equipped with a standard Ubuntu operating system from <code class="language-plaintext highlighter-rouge">Inspur</code>, and I am tasked with configuring it to fulfill the specific requirements of our bioinformatics research. Given that my expertise in Linux is limited, I dedicated several days to this endeavor, and eventually completed the deployment process. The purpose of this guide is to assist researchers facing similar demands in comprehending the steps to configure their servers. Additionally, it aims to address potential issues that may arise during the configuration process, along with their respective solutions.</p> <h2 id="create-a-new-user-with-root-privileges">Create a new user (with root privileges)</h2> <p>Typically, the server comes with a default user named after the vendor, in my instance, <code class="language-plaintext highlighter-rouge">inspur</code>. This user is a regular user but can gain root privileges by executing commands using the <code class="language-plaintext highlighter-rouge">sudo</code> prefix followed by typing their password. To create a custom account with similar privileges, follow these steps:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>useradd <span class="nt">-d</span> <span class="s2">"/home/&lt;user_name&gt;"</span> <span class="nt">-m</span> <span class="nt">-s</span> <span class="s2">"/bin/bash"</span> &lt;user_name&gt;
</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">-d "/volume1/home/&lt;user_name&gt;"</code> will set <code class="language-plaintext highlighter-rouge">/volume1/home/&lt;user_name&gt;</code> as home directory of the new Ubuntu account.</li> <li> <code class="language-plaintext highlighter-rouge">-m</code> will create the user’s home directory.</li> <li> <code class="language-plaintext highlighter-rouge">-s "/bin/bash"</code>will set <code class="language-plaintext highlighter-rouge">/bin/bash</code> as login shell of the new account. This command will create a regular account <code class="language-plaintext highlighter-rouge">&lt;user_name&gt;</code>. If you want <code class="language-plaintext highlighter-rouge">&lt;user_name&gt;</code> to have root privileges, type:</li> </ul> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>useradd <span class="nt">-d</span> <span class="s2">"/home/&lt;user_name&gt;"</span> <span class="nt">-m</span> <span class="nt">-s</span> <span class="s2">"/bin/bash"</span> <span class="nt">-G</span> <span class="nb">sudo</span> &lt;user_name&gt;
</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">-G sudo</code> ensures <code class="language-plaintext highlighter-rouge">&lt;user_name&gt;</code> to have admin access to the system.</li> </ul> <p>To set the password of the new account, conduct:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>passwd &lt;user_name&gt;
</code></pre></div></div> <p>After running the command, you will be prompted to type the password for the new account. Please note that Ubuntu will not display the password you are typing, either explicitly or implicitly (like dots). Just type the password you want to set and press <code class="language-plaintext highlighter-rouge">Enter</code>.</p> <h2 id="change-terminal-prompt-optional">Change terminal prompt (optional)</h2> <p>A beautiful terminal prompt can bring a beautiful day. To change the terminal prompt, execute</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~
vim .bashrc
</code></pre></div></div> <p>You will see a paragraph like this:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code># uncomment for a colored prompt, if the terminal has the capability; turned
# off by default to not distract the user: the focus in a terminal window
# should be on the output of commands, not on the prompt
#force_color_prompt=yes
</code></pre></div></div> <p>Uncomment the last line <code class="language-plaintext highlighter-rouge">#force_color_prompt=yes</code>. Below this paragraph you will also see some codes:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$color_prompt</span><span class="s2">"</span> <span class="o">=</span> <span class="nb">yes</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nv">PS1</span><span class="o">=</span><span class="s1">'${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ '</span>
<span class="k">else
    </span><span class="nv">PS1</span><span class="o">=</span><span class="s1">'${debian_chroot:+($debian_chroot)}\u@\h:\w\$ '</span>
<span class="k">fi</span>
</code></pre></div></div> <p>Modify the first <code class="language-plaintext highlighter-rouge">PS1</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">PS1</span><span class="o">=</span><span class="s1">'\[\033[35m\]\t\[\033[m\]-\[\033[36m\]\u\[\033[m\]@\[\033[32m\]\h:\[\033[33;1m\]\w\[\033[m\]\$ '</span>
</code></pre></div></div> <p>This is my <code class="language-plaintext highlighter-rouge">PS1</code> value. Save the <code class="language-plaintext highlighter-rouge">.bashrc</code> file, close your current terminal and open a new one. The terminal prompt will look like this:</p> <p><span style="color:magenta">23:02:02</span>-<span style="color:skyblue">tdeng</span>@<span style="color:green">inspur-NP5570M5:</span><span style="color:gold">~/data</span>$</p> <h2 id="enable-remote-access">Enable remote access</h2> <p>If you wish to access the server from outside its physical location, you need to enable remote access. In my case, I connected the server to the campus network, allowing me to access it from any location within the campus. To enable remote access, you need to install the <code class="language-plaintext highlighter-rouge">openssh-server</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install </span>openssh-server
</code></pre></div></div> <p>If the firewall <code class="language-plaintext highlighter-rouge">UFW</code> is enabled, make sure to open the SSH port:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>ufw allow ssh
</code></pre></div></div> <p>To test whether you can access the server from a Windows system:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>telnet &lt;remote_ip&gt; &lt;remote_port&gt;
</code></pre></div></div> <p>You can find the IP address using:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ip a
</code></pre></div></div> <p>When you log in to the server using the newly created user with bash, you might encounter an error like this:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/bin/xauth: file /home/&lt;user_name&gt;/.Xauthority does not exist
</code></pre></div></div> <p>Solution:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chown</span> &lt;user_name&gt;:&lt;user_name&gt; <span class="nt">-R</span> /home/&lt;user_name&gt;
</code></pre></div></div> <h2 id="connect-to-github">Connect to GitHub</h2> <p>I suppose you already have a GitHub account. Install <code class="language-plaintext highlighter-rouge">git</code> first:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>git
git <span class="nt">--version</span>
</code></pre></div></div> <p>Then configure certain information about your GitHub account:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git config <span class="nt">--global</span> user.name <span class="s2">"&lt;github_account_name&gt;"</span>
git config <span class="nt">--global</span> user.email <span class="s2">"&lt;github_account_email&gt;"</span>
</code></pre></div></div> <p>Connect to GitHub:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh-keygen <span class="nt">-C</span> <span class="s2">"&lt;github_account_email&gt;"</span> <span class="nt">-t</span> rsa  <span class="c"># default: just press Enter 3 times</span>
<span class="nb">cd</span> ~/.ssh
vim id_rsa.pub  <span class="c"># open the id_rsa.pub file</span>
</code></pre></div></div> <p>Finally, copy the text in <code class="language-plaintext highlighter-rouge">id_rsa.pub</code>, log in GitHub, and create an SSH key at <code class="language-plaintext highlighter-rouge">Settings</code> → <code class="language-plaintext highlighter-rouge">SSH and GPG keys</code> → <code class="language-plaintext highlighter-rouge">New SSH key</code>.</p> <p>Test the connection:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ssh <span class="nt">-T</span> git@github.com
</code></pre></div></div> <h2 id="configure-the-python-environment">Configure the Python environment</h2> <h3 id="install-miniforge">Install miniforge</h3> <p>Instead of <code class="language-plaintext highlighter-rouge">Anaconda</code> I decide to use <code class="language-plaintext highlighter-rouge">Miniforge</code> to manage multiple <code class="language-plaintext highlighter-rouge">Python</code> environments. It has several advantages over Anaconda:</p> <ul> <li>The conda-forge channel is set as the default channel. So you don’t need to type <code class="language-plaintext highlighter-rouge">-c conda-forge</code>.</li> <li>It uses <a href="https://mamba.readthedocs.io/en/latest/index.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">Mamba</code></a>, a very fast package manager (although <code class="language-plaintext highlighter-rouge">Anaconda</code> can also use <code class="language-plaintext highlighter-rouge">Mamba</code>, additional operations to set <code class="language-plaintext highlighter-rouge">conda-libmamba-solver</code> as the dfault solver are required).</li> </ul> <p>You can consider <code class="language-plaintext highlighter-rouge">Miniforge</code> as an alternative to <code class="language-plaintext highlighter-rouge">Anaconda</code>. You can replace the <code class="language-plaintext highlighter-rouge">conda</code> command with <code class="language-plaintext highlighter-rouge">mamba</code> for a better interface, or you can simply keep using the <code class="language-plaintext highlighter-rouge">conda</code> command for a seamless replacement. Below I will only show the former approach.</p> <p>To install <code class="language-plaintext highlighter-rouge">Miniforge</code>, just follow the installation guide in its <a href="https://github.com/conda-forge/miniforge" rel="external nofollow noopener" target="_blank">README</a>. Here I copy the core commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget <span class="s2">"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-</span><span class="si">$(</span><span class="nb">uname</span><span class="si">)</span><span class="s2">-</span><span class="si">$(</span><span class="nb">uname</span> <span class="nt">-m</span><span class="si">)</span><span class="s2">.sh"</span>
bash Miniforge3-<span class="si">$(</span><span class="nb">uname</span><span class="si">)</span>-<span class="si">$(</span><span class="nb">uname</span> <span class="nt">-m</span><span class="si">)</span>.sh
</code></pre></div></div> <p>I prefer install anaconda at <code class="language-plaintext highlighter-rouge">/usr/local/miniforge3</code> so that the environments can be shared by users (but only users with root can modify them). You don’t need to create this folder in advance. During the installation you will have chance to specify the installation directory.</p> <p>To initialize mamba, conduct</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/path/to/mamba init  <span class="c"># /usr/local/miniforge3/bin/mamba in my case</span>
</code></pre></div></div> <p>and reopen the terminal.</p> <h3 id="createdelete-environments">Create/delete environments</h3> <p>I recommend creating new environments and installing site packages with root privileges (<code class="language-plaintext highlighter-rouge">sudo su</code>) to restrict regular users from modifying the environments. If a regular user wants to update an environment, they should contact the system administrator for assistance. If he/she doesn’t and conduct a command secretly like</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba update <span class="nt">--all</span>
</code></pre></div></div> <p>he/she will proceed with the update plan but finally fail with error info:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Confirm changes: <span class="o">[</span>Y/n] y

frozendict                                          49.0kB @  60.0kB/s  0.8s
libzlib                                             61.6kB @  72.1kB/s  0.9s
lzo                                                171.4kB @ 168.4kB/s  1.0s
menuinst                                           137.7kB @ 131.9kB/s  1.0s
libsolv                                            470.7kB @ 324.7kB/s  1.4s
conda                                              961.2kB @ 558.1kB/s  0.9s

Downloading and Extracting Packages:

Preparing transaction: <span class="k">done
</span>Verifying transaction: failed
The current user does not have write permissions to the target environment.
  environment location: /usr/local/miniforge3
  uid: 1000
  gid: 1000



EnvironmentNotWritableError: The current user does not have write permissions to the target environment.
  environment location: /usr/local/miniforge3
  uid: 1000
  gid: 1000
</code></pre></div></div> <p>The commands for creating new environment are:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># create with a specified name</span>
mamba create <span class="nt">--name</span> &lt;new_env_name&gt; <span class="nv">python</span><span class="o">=</span>3.11 <span class="nt">--no-default-packages</span>
<span class="c"># create with a specified location; regular users can use this command to create an environment in their home directory</span>
mamba create <span class="nt">--prefix</span> /path/to/directory <span class="nv">python</span><span class="o">=</span>3.11 <span class="nt">--no-default-packages</span>
</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">--name &lt;new_env_name&gt;</code> will set the name of the new environment.</li> <li> <code class="language-plaintext highlighter-rouge">--prefix /path/to/directory</code> will set the path to the directory where you want to create the environment</li> <li> <code class="language-plaintext highlighter-rouge">python=3.11</code> means mamba will install <code class="language-plaintext highlighter-rouge">Python</code> 3.11 in the new environment.</li> <li> <code class="language-plaintext highlighter-rouge">--no-default-packages</code> will only install <code class="language-plaintext highlighter-rouge">Python</code>. No other site packages will be included.</li> </ul> <p>I did not modify the <code class="language-plaintext highlighter-rouge">base</code> environment and proceeded to create two new environments: <code class="language-plaintext highlighter-rouge">jupyter</code> and <code class="language-plaintext highlighter-rouge">bio</code>. <code class="language-plaintext highlighter-rouge">jupyter</code> only contains packages related to jupyterhub, while <code class="language-plaintext highlighter-rouge">bio</code> encompasses all the necessary packages for research purposes.</p> <p>If you wish to delete an environment for any reason, utilize the following command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># delete with a specified name</span>
mamba remove <span class="nt">--name</span> &lt;env_name&gt; <span class="nt">--all</span>
<span class="c"># delete with a specified location</span>
mamba remove <span class="nt">--prefix</span> /path/to/directory <span class="nt">--all</span>
</code></pre></div></div> <h3 id="install-python-packages">Install Python packages</h3> <h4 id="jupyterhub">JupyterHub</h4> <p>You may want to install <code class="language-plaintext highlighter-rouge">JupyterHub</code>, which serves Jupyter notebook for multiple users.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba <span class="nb">install </span>jupyterhub jupyterlab notebook jupyter-lsp-python jupyterlab-lsp jupyterlab-git
</code></pre></div></div> <p>I recommend to install the <a href="https://github.com/jupyter-lsp/jupyterlab-lsp" rel="external nofollow noopener" target="_blank">jupyterlab-lsp</a>, a powerful coding assistance for JupyterLab. Another useful plugin is <a href="https://github.com/deshaw/jupyterlab-execute-time" rel="external nofollow noopener" target="_blank">jupyterlab-execute-time</a>, which can display cell timings in JupyterLab. Use the following command to install it:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba <span class="nb">install </span>jupyterlab_execute_time
</code></pre></div></div> <p>Refer to this <a href="https://jupyterhub.readthedocs.io/en/stable/tutorial/getting-started/config-basics.html" rel="external nofollow noopener" target="_blank">website</a> for the configuration of JupyterHub.</p> <p>Refer to this <a href="https://professorkazarinoff.github.io/jupyterhub-engr114/systemd/" rel="external nofollow noopener" target="_blank">website</a> for how to run JupyterHub as a system service.</p> <p>The key command to start the service on boot is:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>systemctl <span class="nb">enable </span>jupyterhub
</code></pre></div></div> <p>From version 5.0, you must modify the <code class="language-plaintext highlighter-rouge">jupyterhub_config.py</code> file to grants users who can successfully authenticate access to the Hub. Check <a href="https://jupyterhub.readthedocs.io/en/stable/tutorial/getting-started/authenticators-users-basics.html" rel="external nofollow noopener" target="_blank">this official tutorial</a> out.</p> <h4 id="adddelete-an-environment-as-a-kernel">Add/delete an environment as a kernel</h4> <p>To add an environment as a kernel:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba activate &lt;env_name&gt;  <span class="c"># or /path/to/directory if you create the env with --prefix</span>
mamba <span class="nb">install </span>ipykernel  <span class="c"># if the env doesn't contain this package</span>
python <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--name</span> &lt;kernel_name&gt;
</code></pre></div></div> <p>These commands add <code class="language-plaintext highlighter-rouge">&lt;env_name&gt;</code> environment as a kernel with name <code class="language-plaintext highlighter-rouge">&lt;kernel_name&gt;</code>. If your <code class="language-plaintext highlighter-rouge">Python</code> is 3.11, you may need to modify the last command:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-Xfrozen_modules</span><span class="o">=</span>off <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--name</span> &lt;kernel_name&gt;
</code></pre></div></div> <p>To delete a kernel:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter kernelspec list
jupyter kernelspec uninstall &lt;kernel_name&gt;
</code></pre></div></div> <h4 id="other-packages">Other packages</h4> <p>Our research involves deep learning, so I need to install <code class="language-plaintext highlighter-rouge">pytorch</code> along with other required packages. <a href="https://rapids.ai/" rel="external nofollow noopener" target="_blank">RAPIDS</a> provides a series of packages that utilize GPUs. These packages are easier to install in a fresh environment so I recommend installing them first, following the <a href="https://docs.rapids.ai/install" rel="external nofollow noopener" target="_blank">Installation Guide</a>. <code class="language-plaintext highlighter-rouge">pytorch</code> can be installed simultaneously with the guide.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba <span class="nb">install </span>ipykernel ipywidgets <span class="c"># for Jupyter Notebook</span>
mamba <span class="nb">install </span>lightning  <span class="c"># for deep learning tasks</span>
mamba <span class="nb">install </span>pyro-ppl numpyro funsor arviz  <span class="c"># for probabilistic programming</span>
mamba <span class="nb">install </span>scanpy squidpy omicverse biopython rpy2 opencv   <span class="c"># for biological analysis</span>
mamba <span class="nb">install </span>anndata2ri <span class="nt">-c</span> bioconda  <span class="c"># for conversion between Python and R</span>
mamba <span class="nb">install </span>xgboost lightgbm catboost hdbscan optuna  <span class="c"># for machine learning tasks</span>
</code></pre></div></div> <p>Sometimes you may use <code class="language-plaintext highlighter-rouge">mamba search &lt;package_name&gt;</code> to search for a package with a specific build number. To install a specific version/build of a certain packages, conduct:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mamba <span class="nb">install</span> &lt;package_name&gt;<span class="o">=</span>&lt;version&gt;<span class="o">=</span>&lt;build_string&gt;
</code></pre></div></div> <h4 id="check-pytorchtensorflow">Check pytorch/tensorflow</h4> <p>If you are also a user of <code class="language-plaintext highlighter-rouge">pytorch</code> or <code class="language-plaintext highlighter-rouge">tensorflow</code> and you have one or more available GPU(s), you can execute the following codes to verify whether the GPU(s) can be recognized and utilized by the respective deep learning frameworks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># check pytorch and cuda in use
</span><span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">version</span><span class="p">.</span><span class="n">cuda</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device_count</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">current_device</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># check tensorflow
</span><span class="nf">print</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">list_physical_devices</span><span class="p">(</span><span class="sh">'</span><span class="s">GPU</span><span class="sh">'</span><span class="p">))</span>
</code></pre></div></div> <p>Here I also provide a script to ensure that <code class="language-plaintext highlighter-rouge">pytorch</code> can use the GPU(s) to train and test neural networks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torchvision</span>
<span class="kn">import</span> <span class="n">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>


<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="c1"># define image preprocessing
</span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">Pad</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomHorizontalFlip</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">RandomCrop</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
    <span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">()])</span>

<span class="c1"># download the CIFAR-10 dataset
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nc">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">data/</span><span class="sh">'</span><span class="p">,</span>
                                             <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                             <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                                             <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nc">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="sh">'</span><span class="s">data/</span><span class="sh">'</span><span class="p">,</span>
                                            <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                            <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">())</span>

<span class="c1"># load data
</span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                           <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>            <span class="c1"># number of subprocesses to use for data loading
</span>                                           <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>          <span class="c1"># the data loader will copy Tensors into CUDA pinned memory before returning them
</span>                                           <span class="n">prefetch_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>        <span class="c1"># number of batches loaded in advance by each worker
</span>                                           <span class="n">persistent_workers</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># the data loader will not shutdown the worker processes after a dataset has been consumed once
</span>                                           <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                          <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                          <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                          <span class="n">prefetch_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                          <span class="n">persistent_workers</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># 3x3 convolution kernel
</span><span class="k">def</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                     <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># define the residual block
</span><span class="k">class</span> <span class="nc">ResidualBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">downsample</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="n">downsample</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">downsample</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="n">residual</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># define the structure of ResNet
</span><span class="k">class</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResNet</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="mi">16</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="nf">conv3x3</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layer3</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">make_layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">avg_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">AvgPool2d</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_layer</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">downsample</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">):</span>
            <span class="n">downsample</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="nf">conv3x3</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">))</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">block</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">downsample</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">blocks</span><span class="p">):</span>
            <span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">block</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">layer3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">avg_pool</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">out</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="n">model</span> <span class="o">=</span> <span class="nc">ResNet</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1"># model = nn.DataParallel(model)  # uncomment this line if you have multiple GPUs
</span>

<span class="c1"># define loss function
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># function for the update of learning rate
</span><span class="k">def</span> <span class="nf">update_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

<span class="c1"># train the ResNet
</span><span class="n">total_step</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">curr_lr</span> <span class="o">=</span> <span class="n">learning_rate</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># forward step
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># backward step
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># report every 10 steps
</span>        <span class="nf">if </span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print </span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}</span><span class="sh">"</span>
                   <span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">total_step</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()))</span>

    <span class="c1"># update learning rate
</span>    <span class="nf">if </span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">curr_lr</span> <span class="o">/=</span> <span class="mi">3</span>
        <span class="nf">update_lr</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">curr_lr</span><span class="p">)</span>

<span class="c1"># test the model
</span><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Accuracy of the model on the test images: {} %</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">))</span>
</code></pre></div></div> <p>Note that if you have multiple GPUs, you need to uncomment the line below the code which creates the model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># model = nn.DataParallel(model)
</span></code></pre></div></div> <p>You can use this command to monitor the GPU(s) during training:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch <span class="nt">-n</span> 0.2 nvidia-smi
</code></pre></div></div> <h2 id="configure-the-r-environment">Configure the R environment</h2> <h3 id="install-r">Install R</h3> <p>The simplest way to install <code class="language-plaintext highlighter-rouge">R</code> &gt;= 4.0 is to run</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>r-base
</code></pre></div></div> <p>However, it will not bring you the latest version of <code class="language-plaintext highlighter-rouge">R</code>. To get the latest version of <code class="language-plaintext highlighter-rouge">R</code>, refer to <a href="https://phoenixnap.com/kb/install-r-ubuntu" rel="external nofollow noopener" target="_blank">this website</a> and <a href="https://cran.r-project.org/bin/linux/ubuntu/fullREADME.html" rel="external nofollow noopener" target="_blank">this offical website</a>. Here I copy the core commands:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># update the package list from repositories</span>
<span class="nb">sudo </span>apt update
<span class="c"># install without confirmation</span>
<span class="nb">sudo </span>apt <span class="nb">install </span>software-properties-common dirmngr <span class="nt">-y</span>
<span class="c"># download the R project public key and add it to the trusted list of GPG keys used by apt</span>
wget <span class="nt">-qO-</span> https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc
<span class="c"># verify the key; the fingerprint should be E298A3A825C0D65DFD57CBB651716619E084DAB9</span>
gpg <span class="nt">--show-keys</span> /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc
<span class="c"># add the CRAN repository for your version of Ubuntu to the list of sources apt uses to install packages</span>
<span class="nb">sudo </span>add-apt-repository <span class="s2">"deb https://cloud.r-project.org/bin/linux/ubuntu </span><span class="si">$(</span>lsb_release <span class="nt">-cs</span><span class="si">)</span><span class="s2">-cran40/"</span>
<span class="c"># install R and its development packages</span>
<span class="nb">sudo </span>apt <span class="nb">install </span>r-base r-base-dev <span class="nt">-y</span>
</code></pre></div></div> <h3 id="install-rstudio">Install RStudio</h3> <p>Follow the <a href="https://posit.co/download/rstudio-server/" rel="external nofollow noopener" target="_blank">official installation guide</a>. This should be easier than installing <code class="language-plaintext highlighter-rouge">JupyterHub</code>.</p> <h3 id="install-r-packages">Install R packages</h3> <p>As an example, let’s install one of the most prevalent R package in the field of single-cell genomics, <a href="https://satijalab.org/seurat/index.html" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">Seurat</code></a> (version 5). Before the installation, you need to install some system-level dependencies first:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>build-essential libssl-dev libcurl4-openssl-dev libxml2-dev libfontconfig1-dev libharfbuzz-dev libfribidi-dev libfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev libhdf5-dev libgsl-dev
</code></pre></div></div> <p>Also note that <code class="language-plaintext highlighter-rouge">Seurat</code> requires <code class="language-plaintext highlighter-rouge">R&gt;=4.4</code>, so you’d better install the <a href="#install-r">latest version of R</a>.</p> <p>Then the process of installing <code class="language-plaintext highlighter-rouge">Seurat</code> should be very smooth:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>R
</code></pre></div></div> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chooseCRANmirror</span><span class="p">(</span><span class="n">graphics</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"Seurat"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p>Additional packages can be installed to enhance the functionality of <code class="language-plaintext highlighter-rouge">Seurat</code>. Check the <a href="https://satijalab.org/seurat/articles/install_v5" rel="external nofollow noopener" target="_blank">official intallation tutorial</a> of <code class="language-plaintext highlighter-rouge">Seurat</code> out. If you intend to install an extremely large R package, you’d better set a longer timeout:</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">options</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="m">999</span><span class="p">)</span><span class="w">
</span><span class="n">install.packages</span><span class="p">(</span><span class="s2">"&lt;large_package&gt;"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <p>Other useful R packages are:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">devtools</code> for package development</li> <li> <code class="language-plaintext highlighter-rouge">tidyverse</code> for geneal data analysis</li> <li> <code class="language-plaintext highlighter-rouge">tidyomics</code> for omics data analysis</li> <li> <code class="language-plaintext highlighter-rouge">ComplexHeatmap</code> for visualizing matrices</li> </ul> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">install.packages</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="s2">"devtools"</span><span class="p">,</span><span class="w"> </span><span class="s2">"tidyverse"</span><span class="p">))</span><span class="w">
</span><span class="n">BiocManager</span><span class="o">::</span><span class="n">install</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="s2">"tidyomics"</span><span class="p">,</span><span class="w"> </span><span class="s2">"ComplexHeatmap"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div> <p>When running <code class="language-plaintext highlighter-rouge">devtools::install_github()</code>, you may encounter an error complaining that the API rate limit has been exceeded. The solution to this issue is to create a GitHub token.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">usethis</span><span class="o">::</span><span class="n">create_github_token</span><span class="p">()</span><span class="w">
</span></code></pre></div></div> <p>Run this code in your RStudio console and log in to your GitHub account. Click <code class="language-plaintext highlighter-rouge">Settings</code> → <code class="language-plaintext highlighter-rouge">Developer settings</code> → <code class="language-plaintext highlighter-rouge">Personal access token</code> → <code class="language-plaintext highlighter-rouge">Tokens (classic)</code> (if the browser does not automatically direct you to this page) and generate a token. Run</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gitcreds</span><span class="o">::</span><span class="n">gitcreds_set</span><span class="p">()</span><span class="w">
</span></code></pre></div></div> <p>also in your RStudio console (or in the terminal if you are <code class="language-plaintext highlighter-rouge">sudo</code>) to add the token. The limit should be relaxed and you can continue the installation, and you can see a message like:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using GitHub PAT from the git credential store.
Downloading GitHub repo &lt;github_username&gt;/&lt;repo_name&gt;@&lt;branch_name&gt;
</code></pre></div></div> <h2 id="synchronize-data">Synchronize data</h2> <p>Refer to <a href="https://www.digitalocean.com/community/tutorials/how-to-use-rsync-to-sync-local-and-remote-directories" rel="external nofollow noopener" target="_blank">this website</a> for detailed instructions on how to synchronize data stored on another server.</p> <p>The key command is</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rsync <span class="nt">-r</span> /path/to/sync/ &lt;username&gt;@&lt;remote_host&gt;:&lt;destination_directory&gt;
</code></pre></div></div> <p>which “pushes” all contents in <code class="language-plaintext highlighter-rouge">/path/to/sync/</code> from the system you are logging in to <code class="language-plaintext highlighter-rouge">&lt;destination_directory&gt;</code> in the target system.</p> <p>If you are synchronizing a large file, you may want to monitor the process:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch <span class="nt">-n</span> &lt;time_interval&gt; <span class="nb">du</span> <span class="nt">-sh</span> /path/to/large/file
</code></pre></div></div> <h2 id="install-some-basic-fonts">Install some basic fonts</h2> <p>By default, some basic fonts in Windows are not installed in Linux, such as <code class="language-plaintext highlighter-rouge">Arial</code> and <code class="language-plaintext highlighter-rouge">Times New Roman</code>. These fonts are commonly used in papers and websites, and having them installed can improve the display of figures that expect these fonts to be available. You can install them by:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>msttcorefonts
<span class="nb">rm</span> <span class="nt">-rf</span> ~/.cache/matplotlib
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">msttcorefonts</code> package is a collection of TrueType fonts from Microsoft. The second command clears the matplotlib cache directory located in the hidden <code class="language-plaintext highlighter-rouge">.cache</code> directory in the user’s home directory.</p> <h2 id="troubleshooting">Troubleshooting</h2> <h3 id="driverlibrary-version-mismatch">Driver/library version mismatch</h3> <p>When you run <code class="language-plaintext highlighter-rouge">nvidia-smi</code>, you may get</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Failed to initialize NVML: Driver/library version mismatch
</code></pre></div></div> <p><a href="https://stackoverflow.com/questions/43022843/nvidia-nvml-driver-library-version-mismatch/45319156#45319156" rel="external nofollow noopener" target="_blank">This answer</a> from stackoverflow may help. Briefly you can either reboot or unload the <code class="language-plaintext highlighter-rouge">nvidia</code> module. However, if both the ways can’t help, you need to reinstall the nvidia drivers:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt purge nvidia<span class="k">*</span> libnvidia<span class="k">*</span>
<span class="nb">sudo </span>ubuntu-drivers autoinstall
</code></pre></div></div> <p>and then <code class="language-plaintext highlighter-rouge">sudo reboot</code> your server.</p> <h3 id="upgrade-nvidia-drivers">Upgrade Nvidia drivers</h3> <p>You can upgrade the Nvidia driver by these steps:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># clean the installed version</span>
<span class="nb">sudo </span>apt purge <span class="k">*</span>nvidia<span class="k">*</span> <span class="nt">-y</span>
<span class="nb">sudo </span>apt remove <span class="k">*</span>nvidia<span class="k">*</span> <span class="nt">-y</span>
<span class="nb">sudo rm</span> /etc/apt/sources.list.d/cuda<span class="k">*</span>
<span class="nb">sudo </span>apt autoremove <span class="nt">-y</span> <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt autoclean <span class="nt">-y</span>
<span class="nb">sudo rm</span> <span class="nt">-rf</span> /usr/local/cuda<span class="k">*</span>

<span class="c"># find recommended driver versions</span>
ubuntu-drivers devices  <span class="c"># or sudo apt search nvidia</span>

<span class="c"># install the lastest version (replace `550` with the latest version number)</span>
<span class="nb">sudo </span>apt <span class="nb">install </span>libnvidia-common-550-server libnvidia-gl-550-server nvidia-driver-550-server <span class="nt">-y</span>

<span class="c"># reboot</span>
<span class="nb">sudo </span>reboot now
</code></pre></div></div> <p>After reboot, you can check whether the new driver works by <code class="language-plaintext highlighter-rouge">nvidia-smi</code> ( although you may be required to also install <code class="language-plaintext highlighter-rouge">nvidia-utils-550-server</code>). Theoretically the command <code class="language-plaintext highlighter-rouge">nvidia-smi</code> should work, but you may still get an error message</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.
</code></pre></div></div> <p>even you have installed the latest driver. In this case you can try reinstalling kernel headers:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">--reinstall</span> linux-headers-<span class="si">$(</span><span class="nb">uname</span> <span class="nt">-r</span><span class="si">)</span>
</code></pre></div></div> <p>If you encounter some errors like <code class="language-plaintext highlighter-rouge">cc: error: unrecognized command-line option ‘-ftrivial-auto-var-init=zero’</code>, you can use <code class="language-plaintext highlighter-rouge">gcc 12</code> instead of <code class="language-plaintext highlighter-rouge">gcc 11</code> by</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install </span>gcc-12
<span class="nb">sudo </span>update-alternatives <span class="nt">--install</span> /usr/bin/gcc gcc /usr/bin/gcc-12 12
</code></pre></div></div> <p>After the headers are reinstalled, you need to <code class="language-plaintext highlighter-rouge">sudo reboot</code> the server. Then <code class="language-plaintext highlighter-rouge">nvidia-smi</code> should work now.</p> <h3 id="cant-use-latex-fonts-in-matplotlib-figures">Can’t use $\LaTeX$ fonts in <code class="language-plaintext highlighter-rouge">matplotlib</code> figures</h3> <p>According to the <a href="https://matplotlib.org/stable/gallery/text_labels_and_annotations/tex_demo.html" rel="external nofollow noopener" target="_blank">demo</a> of <code class="language-plaintext highlighter-rouge">matplotlib</code>, you should be able to use $\LaTeX$ fonts by setting</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="sh">'</span><span class="s">text.usetex</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
</code></pre></div></div> <p>However, if you don’t install $\LaTeX$ in your system, you will get an error:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RuntimeError: Failed to process string with tex because latex could not be found
</code></pre></div></div> <p>So just install $\LaTeX$:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>texlive texlive-latex-extra texlive-fonts-recommended dvipng cm-super
</code></pre></div></div> <p>Now, your server should be well-suited for your bioinformatics research and you know what to do when things go wrong. Enjoy it!</p> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Tao Deng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: July 09, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>